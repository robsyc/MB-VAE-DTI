#!/bin/bash

# Basic parameters
#PBS -N grid_experiment         ## Job name
#PBS -l nodes=1:ppn=8:gpus=1    ## nodes, processors per node (ppn=all to get a full node), GPUs (H100 with 32gb)
#PBS -l walltime=48:00:00       ## Max time your job will run (no more than 72:00:00)
#PBS -l mem=24gb                ## If not used, memory will be available proportional to the max amount
#PBS -m abe                     ## Email notifications (abe=aborted, begin and end)

# Load the necessary modules
ml load PyTorch/2.1.2-foss-2023a-CUDA-12.1.1
ml load PyTorch-Lightning/2.2.1-foss-2023a-CUDA-12.1.1
ml load Hydra/1.3.2-foss-2023a-with-plugins
ml load wandb/0.16.1-GCC-12.3.0
ml load PyTorch-Geometric/2.5.0-foss-2023a-PyTorch-2.1.2-CUDA-12.1.1
ml load lifelines/0.28.0-gfbf-2023a
ml load RDKit/2024.03.3-foss-2023a
# pip install torcheval h5torch

# Change working directory to the location where the job was submmitted
cd /data/gent/454/vsc45450/thesis/training

echo "Running job $batch_index out of $total_batches"

python3 run.py --model baseline --phase finetune --dataset DAVIS --split cold --gridsearch --batch_index $batch_index --total_batches $total_batches

# Baseline Davis -> 5GB mem used