model:
  embedding_dim: 768
  encoder_type: "transformer"
  encoder_kwargs:
    hidden_dim: 512
    n_layers: 2
    
  fusion_kwargs:
    hidden_dim: 512
    n_layers: 2
  
  # Phase-specific parameters
  finetune_score: null
  checkpoint_path: null

training:
  learning_rate: 0.001
  
data:
  batch_size: 32

logging:
  experiment_name: "multi_output_pretrain_rand"
  save_dir: "outputs/multi_output/pretrain/rand"

gridsearch:
  model.embedding_dim: [512, 768, 1024]
  model.encoder_type: ["resnet", "transformer"]
  model.encoder_kwargs.hidden_dim: [256, 512]
  model.encoder_kwargs.n_layers: [2, 3]
  
  model.fusion_kwargs.hidden_dim: [128, 256]
  model.fusion_kwargs.n_layers: [1, 2]
  
  training.learning_rate: [0.0001, 0.001]
  data.batch_size: [16, 32] 