model:
  name: "multi_hybrid_pretrain_target"
  
  encoder_type: "resnet"
  encoder_kwargs:
    hidden_dim: 256
    output_dim: 512
    n_layers: 2
  
  aggregator_type: "attentive"
  aggregator_kwargs:
    hidden_dim: 512
    output_dim: 768
    n_layers: 2
  
  # Phase-specific parameters
  phase: "pretrain_target"  # Target branch pretraining
  checkpoint_path: null

training:
  learning_rate: 0.001
  
data:
  batch_size: 128             # large bs for contrastive learning
  h5_path: "targets.h5torch"  # Use target pretraining dataset
  load_in_memory: false
  drug_features: null
  target_features: ["EMB-ESM", "EMB-NT"] 
  
logging:
  experiment_name: "multi_hybrid_pretrain_target"
  save_dir: "outputs/multi_hybrid/pretrain/target"