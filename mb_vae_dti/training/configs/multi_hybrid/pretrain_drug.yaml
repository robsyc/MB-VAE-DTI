training:
  learning_rate: 0.001
  
data:
  batch_size: 128                      # large bs for contrastive learning
  h5_path: "data/input/drugs.h5torch"  # Use drug pretraining dataset
  drug_features: ["EMB-BiomedGraph", "EMB-BiomedImg", "EMB-BiomedText"]
  target_features: null
  load_in_memory: false

logging:
  experiment_name: "multi_hybrid_pretrain_drug"
  save_dir: "data/results/multi_hybrid/pretrain/drug"