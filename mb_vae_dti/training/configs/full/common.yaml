# Full DTI Model Common Configuration
#   Combines multi-modal inputs with multi-output predictions & drug reconstruction
#   Supports individual branch pretraining with contrastive learning
#   These configurations are shared across all training phases

model:
  embedding_dim: 1024
  encoder_type: "transformer"
  encoder_kwargs:
    dropout: 0.1
    activation: "gelu"
    hidden_dim: 512
    n_layers: 3

  aggregator_type: "attentive"
  aggregator_kwargs:
    dropout: 0.1
    activation: "gelu"

  fusion_kwargs:
    dropout: 0.1
    output_dim: 256
    hidden_dim: 512
    n_layers: 3

  dti_head_kwargs:
    hidden_dim: 128
    dropout: 0.1
  
  infonce_head_kwargs:
    output_dim: 128
    temperature: 0.07

  graph_transformer_kwargs:
    n_layers: 5
    input_dims: {'X': 16, 'E': 5, 'y': 1037} # after augmentation
    output_dims: {'X': 8, 'E': 5, 'y': 1024} # y_output_dim == embedding_dim
    hidden_mlp_dims: {'X': 256, 'E': 128, 'y': 1024}
    hidden_dims : {'dx': 256, 'de': 64, 'dy': 1024, 'n_head': 8, 'dim_ffX': 256, 'dim_ffE': 128, 'dim_ffy': 1024}
  
  contrastive_weight: 0.1
  complexity_weight: 0.001
  accuracy_weight: 1.0
  reconstruction_weight: 1.0
  lambda_train: [1, 5, 0]

diffusion:
  diffusion_steps: 500
  num_samples_to_generate: 100

training:
  scheduler: "one_cycle"  # "const", "step", "one_cycle", or "cosine"
    
data:
  h5_path: "data/input/dti.h5torch"
  drug_features: ["EMB-BiomedGraph", "EMB-BiomedImg", "EMB-BiomedText"]
  target_features: ["EMB-ESM", "EMB-NT"] 