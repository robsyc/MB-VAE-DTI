training:
  learning_rate: 0.00001
  max_epochs: 32
  early_stopping_patience: 12
  gradient_clip_val: null      # Disabled - try without first (https://medium.com/@kaveh.kamali/a-beginners-guide-to-gradient-clipping-with-pytorch-lightning-c394d28e2b69)
  weight_decay: 0.0001         # Increase if overfitting, decrease if underfitting
  scheduler: "cosine"       # "const", "step", "one_cycle", or "cosine"

loss:
  weights: [1, 0, 0, 0] # for accuracy, complexity, contrastive, reconstruction
  # dti_weights: [1.0, 0.903964, 0.282992, 0.755172] # for binary, pKd, pKi, KIBA (real values are weighted ~ representation in train)
  diff_weights: [1, 5] # for nodes X and edges E
  # contrastive_temp: 0.1

data:
  batch_size: 16
  num_workers: 4               # increase for multi-GPU training ~ depends if bottleneck in data loading or processing
  pin_memory: true             # Set to false for CPU training (true for GPU)
  shuffle_train: true
  drop_last: false             # no dropping of last batch when it is incomplete
  load_in_memory: true
  h5_path: "data/input/dti.h5torch" # also drugs.h5torch & targets.h5torch
  drug_features: ["FP-Morgan"] # "FP-Morgan", "EMB-BiomedGraph", "EMB-BiomedImg", "EMB-BiomedText"
  target_features: ["FP-ESP"] # "FP-ESP", "EMB-ESM", "EMB-NT"

logging:
  project_name: "MB-VAE-DTI"
  log_every_n_steps: 50
  use_wandb: true
  save_dir: "data/results/full/KIBA/cold"
  
hardware:
  gpus: 1                      # run on cpu or set to 1 for gpu
  precision: 32-true           # https://lightning.ai/docs/pytorch/stable/common/precision_basic.html
  deterministic: true          # Set to False when on CPU
  seed: 42

model:
  embedding_dim: 1024  # output representation of a drug/target encoding brach
  hidden_dim: 512      # within-network dimensionality
  factor: 4            # expansion / contraction factor in mlp and att blocks respectively
  n_layers: 3          # number of residual layers in mlp and att blocks
  activation: "relu"
  dropout: 0.2
  bias: true
  # checkpoint_path: "data/results/full/KIBA/cold/full_finetune_KIBA_cold/checkpoints/best_model.ckpt"
  checkpoint_path: "data/results/full/pretrain/cold/full_train_cold/checkpoints/best.ckpt"
  drug_checkpoint_path: null
  target_checkpoint_path: null

  encoder_type: "transformer" # transformer or resnet
  aggregator_type: "attentive" # attentive or concat

  graph_transformer_kwargs:
    n_layers: 5
    input_dims: {'X': 16, 'E': 5, 'y': 1037} # after augmentation
    output_dims: {'X': 8, 'E': 5, 'y': 1024} # y_output_dim == embedding_dim
    hidden_mlp_dims: {'X': 256, 'E': 128, 'y': 2048}
    hidden_dims : {'dx': 256, 'de': 64, 'dy': 1024, 'n_head': 8, 'dim_ffX': 256, 'dim_ffE': 128, 'dim_ffy': 1024}

  diffusion_steps: 500
  sample_every_val: 5
  val_samples_per_embedding: 5
  test_samples_per_embedding: 10

gridsearch:
  training.learning_rate: [0.000001, 0.000005]
  data.batch_size: [8, 16]
  model.dropout: [0.2, 0.3]