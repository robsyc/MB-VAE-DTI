model:
  embedding_dim: 768
  encoder_type: "resnet"
  encoder_kwargs:
    hidden_dim: 256
    n_layers: 3

training:
  learning_rate: 0.001

data:
  batch_size: 32
  
logging:
  experiment_name: "baseline_KIBA_cold"
  save_dir: "data/results/baseline/KIBA/cold"

gridsearch: # 2000s (40 mins) for a run, max 6 gb mem used on Davis -> 8 jobs, 48 hours each
  model.embedding_dim: [512, 768, 1024]
  model.encoder_type: ["resnet", "transformer"]
  model.encoder_kwargs.hidden_dim: [128, 256, 512]
  model.encoder_kwargs.n_layers: [1, 2, 3]

  training.learning_rate: [0.0005, 0.001, 0.002]
  data.batch_size: [16, 32, 64]