{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/home/robsyc/Desktop/thesis/MB-VAE-DTI\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04:19:00 - INFO - Old pandas version detected. Patching DataFrame.map to DataFrame.applymap\n"
     ]
    }
   ],
   "source": [
    "import h5torch\n",
    "\n",
    "from mb_vae_dti.processing import load_h5torch_DTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<KeysViewHDF5 ['Drug_ID', 'Drug_InChIKey', 'Drug_SMILES']>\n",
      "1\n",
      "<KeysViewHDF5 ['Target_AA', 'Target_DNA', 'Target_Gene_name', 'Target_ID', 'Target_RefSeq_ID', 'Target_UniProt_ID']>\n",
      "central\n",
      "<KeysViewHDF5 ['data', 'indices']>\n",
      "unstructured\n",
      "<KeysViewHDF5 ['Y_KIBA', 'Y_pKd', 'Y_pKi', 'in_BindingDB_Kd', 'in_BindingDB_Ki', 'in_DAVIS', 'in_KIBA', 'in_Metz', 'split_cold', 'split_rand']>\n"
     ]
    }
   ],
   "source": [
    "file_path = \"data/processed/data.h5torch\"\n",
    "with h5torch.File(file_path, \"r\") as f:\n",
    "    for key in f.keys():\n",
    "        print(key)\n",
    "        print(f[key].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Unable to synchronously open object (object 'Target_fp' doesn't exist)\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5torch\u001b[38;5;241m.\u001b[39mFile(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1/Target_fp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m100\u001b[39m][:\u001b[38;5;241m20\u001b[39m])\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(f[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1/Target_fp\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m100\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/mbvae_env/lib/python3.11/site-packages/h5py/_hl/group.py:357\u001b[0m, in \u001b[0;36mGroup.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid HDF5 object reference\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m)):\n\u001b[0;32m--> 357\u001b[0m     oid \u001b[38;5;241m=\u001b[39m \u001b[43mh5o\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessing a group is done with bytes or str, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(name)))\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5o.pyx:257\u001b[0m, in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Unable to synchronously open object (object 'Target_fp' doesn't exist)\""
     ]
    }
   ],
   "source": [
    "with h5torch.File(file_path, \"r\") as f:\n",
    "    print(f[\"1/Target_fp\"][100][:20])\n",
    "    print(f[\"1/Target_fp\"][100].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using boolean mask for mapping (7145 indices)\n",
      "Verified alignment: all unstructured data has 396469 elements\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'central': True,\n",
       " '0/Drug_ID': 'D000028',\n",
       " '0/Drug_InChIKey': 'XZXHXSATPCNXJR-ZIADKAODSA-N',\n",
       " '0/Drug_SMILES': 'COC(=O)c1ccc2c(c1)NC(=O)C2=C(Nc1ccc(N(C)C(=O)CN2CCN(C)CC2)cc1)c1ccccc1',\n",
       " '1/Target_AA': 'MEQPPAPKSKLKKLSEDSLTKQPEEVFDVLEKLGEGSYGSVFKAIHKESGQVVAIKQVPVESDLQEIIKEISIMQQCDSPYVVKYYGSYFKNTDLWIVMEYCGAGSVSDIIRLRNKTLIEDEIATILKSTLKGLEYLHFMRKIHRDIKAGNILLNTEGHAKLADFGVAGQLTDTMAKRNTVIGTPFWMAPEVIQEIGYNCVADIWSLGITSIEMAEGKPPYADIHPMRAIFMIPTNPPPTFRKPELWSDDFTDFVKKCLVKNPEQRATATQLLQHPFIKNAKPVSILRDLITEAMEIKAKRHEEQQRELEEEEENSDEDELDSHTMVKTSVESVGTMRATSTMSEGAQTMIEHNSTMLESDLGTMVINSEDEEEEDGTMKRNATSPQVQRPSFMDYFDKQDFKNKSHENCNQNMHEPFPMSKNVFPDNWKVPQDGDFDFLKNLSLEELQMRLKALDPMMEREIEELRQRYTAKRQPILDAMDAKKRRQQNF',\n",
       " '1/Target_DNA': 'AGTAAACTAAAAAAGCTGAGTGAAGACAGTTTGACTAAGCAGCCTGAAGAAGTTTTTGATGTATTAGAGAAGCTTGGAGAAGGGTCTTATGGAAGTGTATTTAAAGCAATACACAAGGAATCCGGTCAAGTTGTCGCAATTAAACAAGTACCTGTTGAATCAGATCTTCAGGAAATAATCAAAGAAATTTCCATAATGCAGCAATGTGACAGCCCATATGTTGTAAAGTACTATGGCAGTTATTTTAAGAATACAGACCTCTGGATTGTTATGGAGTACTGTGGCGCTGGCTCTGTCTCAGACATAATTAGATTACGAAACAAGACATTAATAGAAGATGAAATTGCAACCATTCTTAAATCTACATTGAAAGGACTAGAATATTTGCACTTTATGAGAAAAATACACAGAGATATAAAAGCTGGAAATATTCTCCTCAATACAGAAGGACATGCAAAATTGGCAGATTTTGGAGTGGCTGGTCAGTTAACAGATACAATGGCAAAACGCAATACTGTAATAGGAACTCCATTTTGGATGGCTCCTGAGGTGATTCAAGAAATAGGCTATAACTGTGTGGCCGACATCTGGTCCCTTGGCATTACTTCTATAGAAATGGCTGAAGGAAAACCTCCTTATGCTGATATACATCCAATGAGGGCTATTTTTATGATTCCCACAAATCCACCACCAACATTCAGAAAGCCAGAACTTTGGTCCGATGATTTCACCGATTTTGTTAAAAAGTGTTTGGTGAAGAATCCTGAGCAGAGAGCTACTGCAACACAACTTTTACAGCATCCTTTTATCAAGAATGCCAAACCTGTATCAATATTAAGAGACCTGATCACAGAAGCTATGGAGATCAAAGCTAAAAGACATGAGGAACAGCAACGAGAATTGGAAGAGGAAGAAGAAAATTCGGATGAAGATGAGCTGGATTCCCACACCATGGTGAAGACTAGTGTGGAGAGTGTGGGCACCATGCGGGCCACAAGCACGATGAGTGAAGGGGCCCAGACCATGATTGAACATAATAGCACGATGTTGGAATCCGACTTGGGGACCATGGTGATAAACAGTGAGGATGAGGAAGAAGAAGATGGAACTATGAAAAGAAATGCAACCTCACCACAAGTACAAAGACCATCTTTCATGGACTACTTTGATAAGCAAGACTTCAAGAATAAGAGTCACGAAAACTGTAATCAGAACATGCATGAACCCTTCCCTATGTCCAAAAACGTTTTTCCTGATAACTGGAAAGTTCCTCAAGATGGAGACTTTGACTTTTTGAAAAATCTAAGTTTAGAAGAACTACAGATGCGGTTAAAAGCACTGGACCCCATGATGGAACGGGAGATAGAAGAACTTCGTCAGAGATACACTGCGAAAAGACAGCCCATTCTGGATGCGATGGATGCAAAGAAAAGAAGGCAGCAAAACTTT',\n",
       " '1/Target_Gene_name': 'STK3',\n",
       " '1/Target_ID': 'T000154',\n",
       " '1/Target_RefSeq_ID': 'NM_001256312',\n",
       " '1/Target_UniProt_ID': 'Q13188',\n",
       " 'unstructured/Y_pKd': 7.419075,\n",
       " 'unstructured/Y_pKi': nan,\n",
       " 'unstructured/Y_KIBA': nan}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_davis_metz = load_h5torch_DTI(\n",
    "    setting=\"split_cold\",\n",
    "    split=\"test\",\n",
    "    datasets=[\"in_DAVIS\", \"in_Metz\", \"in_BindingDB_Kd\"]\n",
    ")\n",
    "test_davis_metz[153]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MEQPPAPKSKLKKLSEDSLTKQPEEVFDVLEKLGEGSYGSVFKAIHKESGQVVAIKQVPVESDLQEIIKEISIMQQCDSPYVVKYYGSYFKNTDLWIVMEYCGAGSVSDIIRLRNKTLIEDEIATILKSTLKGLEYLHFMRKIHRDIKAGNILLNTEGHAKLADFGVAGQLTDTMAKRNTVIGTPFWMAPEVIQEIGYNCVADIWSLGITSIEMAEGKPPYADIHPMRAIFMIPTNPPPTFRKPELWSDDFTDFVKKCLVKNPEQRATATQLLQHPFIKNAKPVSILRDLITEAMEIKAKRHEEQQRELEEEEENSDEDELDSHTMVKTSVESVGTMRATSTMSEGAQTMIEHNSTMLESDLGTMVINSEDEEEEDGTMKRNATSPQVQRPSFMDYFDKQDFKNKSHENCNQNMHEPFPMSKNVFPDNWKVPQDGDFDFLKNLSLEELQMRLKALDPMMEREIEELRQRYTAKRQPILDAMDAKKRRQQNF'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = test_davis_metz[153]['1/Target_AA']\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4170,)\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from mb_vae_dti.processing.embed_helper import get_target_fingerprint\n",
    "\n",
    "aa_fp = get_target_fingerprint(aa)\n",
    "print(aa_fp.shape)\n",
    "print(aa_fp[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5torch\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from typing import Callable, Tuple, Optional, List, Union\n",
    "\n",
    "def add_processed_feature(\n",
    "    file_path: str,\n",
    "    entity_path: str,\n",
    "    process_func: Callable[[Union[str, List[str]]], Union[np.ndarray, List[np.ndarray]]],\n",
    "    feature_name: str,\n",
    "    batch_size: int = 1,\n",
    "    dtype_save: str = \"float32\",\n",
    "    dtype_load: str = \"float32\",\n",
    "    overwrite: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Generic function to add processed features to h5torch file with batch processing support.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the h5torch file\n",
    "        entity_path (str): Path to entity in h5torch file (e.g. \"1/Target_AA\")\n",
    "        process_func (Callable): Function to process entities, can handle batches if batch_size > 1\n",
    "        feature_name (str): Name for the new feature\n",
    "        batch_size (int): Number of entities to process at once (1 for no batching)\n",
    "        dtype_save (str): Data type to save as\n",
    "        dtype_load (str): Data type to load as\n",
    "        overwrite (bool): Whether to overwrite existing feature if it exists\n",
    "    \"\"\"\n",
    "    # Parse entity path to get axis\n",
    "    parts = entity_path.split(\"/\")\n",
    "    axis = parts[0]\n",
    "    \n",
    "    # Open the file in append mode\n",
    "    f = h5torch.File(file_path, \"a\")\n",
    "    \n",
    "    # Get the axis number if it's a digit\n",
    "    axis_num = None\n",
    "    if axis.isdigit():\n",
    "        axis_num = int(axis)\n",
    "    \n",
    "    # Check if feature exists and should be overwritten\n",
    "    feature_path = f\"{axis}/{feature_name}\"\n",
    "    if feature_path in f:\n",
    "        if overwrite:\n",
    "            print(f\"Found existing {feature_name}, deleting it\")\n",
    "            del f[feature_path]\n",
    "        else:\n",
    "            print(f\"Feature {feature_name} already exists and overwrite=False. Skipping.\")\n",
    "            f.close()\n",
    "            return\n",
    "    \n",
    "    # Get all entity values\n",
    "    entities = f[entity_path][:]\n",
    "    \n",
    "    # Determine if entities need decoding (from bytes to str)\n",
    "    needs_decoding = isinstance(entities[0], bytes)\n",
    "    \n",
    "    # Process a sample to determine output shape\n",
    "    sample_entity = entities[0].decode('utf-8') if needs_decoding else entities[0]\n",
    "    sample_result = process_func(sample_entity)\n",
    "    result_shape = sample_result.shape if hasattr(sample_result, 'shape') else (1,)\n",
    "    \n",
    "    # Create array to store processed results\n",
    "    n_entities = len(entities)\n",
    "    output_array = np.zeros((n_entities, *result_shape), dtype=np.float32)\n",
    "    \n",
    "    # Adjust batch size if needed\n",
    "    effective_batch_size = min(batch_size, n_entities)\n",
    "    \n",
    "    print(f\"Processing {n_entities} entities with batch size {effective_batch_size}, output dimension {result_shape}\")\n",
    "    \n",
    "    # Process entities in batches\n",
    "    num_batches = (n_entities + effective_batch_size - 1) // effective_batch_size\n",
    "    \n",
    "    for batch_idx in tqdm.tqdm(range(num_batches), desc=f\"Processing {feature_name} batches\"):\n",
    "        # Get batch indices\n",
    "        start_idx = batch_idx * effective_batch_size\n",
    "        end_idx = min(start_idx + effective_batch_size, n_entities)\n",
    "        \n",
    "        # Prepare batch of entities\n",
    "        batch_entities = entities[start_idx:end_idx]\n",
    "        \n",
    "        # Decode if needed\n",
    "        if needs_decoding:\n",
    "            batch_entities = [e.decode('utf-8') for e in batch_entities]\n",
    "        \n",
    "        # Process batch\n",
    "        if effective_batch_size == 1:\n",
    "            # Single item processing\n",
    "            batch_results = [process_func(batch_entities[0])]\n",
    "        else:\n",
    "            # True batch processing\n",
    "            batch_results = process_func(batch_entities)\n",
    "            if not isinstance(batch_results, list):\n",
    "                # If process_func returns a single array for the whole batch\n",
    "                # Split it into individual results\n",
    "                batch_results = [batch_results[i] for i in range(len(batch_entities))]\n",
    "        \n",
    "        # Store results\n",
    "        for i, result in enumerate(batch_results):\n",
    "            idx = start_idx + i\n",
    "            if idx >= n_entities:\n",
    "                break\n",
    "                \n",
    "            if result_shape == (1,):  # Handle scalar results\n",
    "                output_array[idx, 0] = result\n",
    "            else:\n",
    "                output_array[idx] = result\n",
    "    \n",
    "    # Register the processed features\n",
    "    f.register(\n",
    "        output_array, \n",
    "        mode=\"N-D\", \n",
    "        axis=axis_num if axis_num is not None else axis, \n",
    "        name=feature_name, \n",
    "        dtype_save=dtype_save, \n",
    "        dtype_load=dtype_load\n",
    "    )\n",
    "    \n",
    "    # Close the file\n",
    "    f.close()\n",
    "    print(f\"Successfully added {feature_name} to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2047 entities with batch size 32, output dimension (4170,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Target_fp batches: 100%|██████████| 64/64 [00:15<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added Target_fp to data/processed/data.h5torch\n",
      "Processing 149962 entities with batch size 1, output dimension (2048,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Drug_fp batches: 100%|██████████| 149962/149962 [02:12<00:00, 1135.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added Drug_fp to data/processed/data.h5torch\n"
     ]
    }
   ],
   "source": [
    "from mb_vae_dti.processing.embed_helper import get_target_fingerprint\n",
    "\n",
    "# Example 1: For a function that can process batches:\n",
    "def batch_target_fingerprint(sequences):\n",
    "    \"\"\"Process multiple sequences at once\"\"\"\n",
    "    if isinstance(sequences, str):\n",
    "        return get_target_fingerprint(sequences)\n",
    "    else:\n",
    "        # Process a batch of sequences\n",
    "        return [get_target_fingerprint(seq) for seq in sequences]\n",
    "\n",
    "# Using batch processing (process 32 sequences at a time)\n",
    "add_processed_feature(\n",
    "    file_path=\"data/processed/data.h5torch\",\n",
    "    entity_path=\"1/Target_AA\",\n",
    "    process_func=batch_target_fingerprint,\n",
    "    feature_name=\"Target_fp\",\n",
    "    batch_size=32,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Example 2: If you had a drug fingerprint function\n",
    "from mb_vae_dti.processing.embed_helper import get_drug_fingerprint\n",
    "\n",
    "add_processed_feature(\n",
    "    file_path=\"data/processed/data.h5torch\",\n",
    "    entity_path=\"0/Drug_SMILES\",\n",
    "    process_func=get_drug_fingerprint,\n",
    "    feature_name=\"Drug_fp\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mbvae_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
