# Multi-branch Variational Encoders for Drug-target Interaction Prediction (MB-VAE-DTI)

A machine learning framework for predicting drug-target interactions using multi-branch variational autoencoders leveraging pre-computed embeddings and target-conditioned discrete-diffusion drug-decoding.

## Project Overview

This project implements a novel approach to the dyadic Drug-Target Interaction (DTI) prediction problem using multi-branch variational autoencoders and a diffusion-based drug decoder based on [DiGress](https://github.com/cvignac/DiGress) and [DiffMS](https://github.com/coleygroup/DiffMS). The framework supports multiple embedding strategies for both drugs (molecules) and targets (proteins), allowing for flexible and powerful representation learning.

Key features:
- Multiple drug representations: Morgan fingerprints, graph, image and text (smiles) from [biomed-multi-view](https://github.com/BiomedSciAI/biomed-multi-view)
- Multiple protein representations: ESPF fingerprints, amino acid ([ESM](https://github.com/facebookresearch/esm)) and DNA sequences ([nucleotide-transformer](https://github.com/instadeepai/nucleotide-transformer))
- Variational (with discrete-diffusion decoding) and non-variational encoder architectures
- Inspection of latent spaces & exploration of generative capabilities
- Comprehensive evaluation on standard DTI datasets through [tdc](https://tdcommons.ai/) (DAVIS, KIBA, BindingDB and Metz) and a new aggregated dataset of Â±400k interactions & pre-computed embeddings, as well as a pre-training strategy for the full drug branch.

## Repository Structure (simplified)

```
MB-VAE-DTI/
â”œâ”€â”€ mb_vae_dti/                 # Main Python package
â”‚   â”œâ”€â”€ loading/                # Loading and preprocessing datasets
â”‚   â”‚   â”œâ”€â”€ datasets.py         # Loading, merging, filtering and saving initial DTI datasets
â”‚   â”‚   â”œâ”€â”€ annotation.py       # Addition of DNA sequences, InChI keys, etc.
â”‚   â”‚   â””â”€â”€ visualization.py    # Plotting metrics for loaded data
|   | 
â”‚   â”œâ”€â”€ processing/             # Embedding & h5torch file creation
â”‚   â”‚   â”œâ”€â”€ embedding.py        # Embedding generation
â”‚   â”‚   â”œâ”€â”€ h5factory.py        # h5torch file creation
â”‚   â”‚   â””â”€â”€ split.py            # Dataset splitting utilities
|   |
â”‚   â”œâ”€â”€ training/               # Model training (in progress)
â”‚   â”‚   â”œâ”€â”€ models.py           # Model architecture definitions
â”‚   â”‚   â”œâ”€â”€ components.py       # Reusable model components
â”‚   â”‚   â””â”€â”€ trainer.py          # Training loop implementation
|   |
â”‚   â””â”€â”€ validating/             # Validation and analysis (in progress)
â”‚       â”œâ”€â”€ metrics.py          # Accuracy metrics computation
â”‚       â””â”€â”€ visualization.py    # Result plotting and visualization
|
â”œâ”€â”€ external/                   # External dependencies
â”‚   â”œâ”€â”€ rdMorganFP/             # Drug fingerprinting utilities
â”‚   â”œâ”€â”€ biomed-multi-view/      # Multiple drug representation models (graph image, text)
â”‚   â”œâ”€â”€ ESPF/                   # Protein fingerprinting utilities
â”‚   â”œâ”€â”€ ESM/                    # Protein language model (ESM-C 6B)
â”‚   â”œâ”€â”€ nucleotide-transformer/ # DNA sequence transformer (nucleotide-transformer)
â”‚   â””â”€â”€ run_embeddings.sh       # Script to run embedding generation
|
â”œâ”€â”€ data/                       # Data directory (gitignored, download from [here](https://test.com))
â”‚   â”œâ”€â”€ source/                 # Original datasets, populated by loading notebook & tdc
â”‚   â”œâ”€â”€ processed/              # Processed datasets & embeddings
â”‚   â”œâ”€â”€ input/                  # Input datasets (h5torch)
â”‚   â”œâ”€â”€ images/                 # Generated plots and visualizations
â”‚   â”œâ”€â”€ checkpoints/            # Model checkpoints
â”‚   â””â”€â”€ results/                # Model outputs and analysis
|
â”œâ”€â”€ notebooks/                  # Jupyter notebooks for reproducing experiments
â”‚   â”œâ”€â”€ loading.ipynb           # Data loading, pre-processing and exploration
â”‚   â”œâ”€â”€ processing.ipynb        # Embedding generation and h5torch file creation
â”‚   â”œâ”€â”€ training.ipynb          # Model inspection and training processes (in progress)
â”‚   â””â”€â”€ validating.ipynb        # Result analysis and validation (in progress)
|
â”œâ”€â”€ scripts/                    # Scripts for running experiments on HPC
â”‚   â”œâ”€â”€ configs/                # Configuration files
â”‚   â”‚   â”œâ”€â”€ pretrain.json
â”‚   â”‚   â”œâ”€â”€ train.json
â”‚   â”‚   â””â”€â”€ valid.json
â”‚   â”œâ”€â”€ embedding.sh            # Shell script for embedding generation
â”‚   â”œâ”€â”€ pretrain.sh             # Shell script for pretraining
â”‚   â”œâ”€â”€ train.sh                # Shell script for training
â”‚   â”œâ”€â”€ validate.sh             # Shell script for validation
â”‚   â””â”€â”€ hpc.pbs                 # PBS script with batch indexing
|
â”œâ”€â”€ setup.py                    # Package installation script
â”œâ”€â”€ environment.yml             # Conda environment specification
â””â”€â”€ README.md                   # Project documentation
```

## Installation

### Prerequisites

- Python 3.9 or higher
- CUDA-compatible GPU (recommended)
- Conda package manager

### Setup

1. **Clone the repository**
   ```bash
   git clone https://github.com/robsyc/MB-VAE-DTI.git
   cd MB-VAE-DTI
   ```

2. **Run the setup script**
   ```bash
   ./setup_env.sh
   ```
   This script will:
   - Create a conda environment with the necessary CUDA dependencies
   - Install all required Python packages
   - Set up the package in development mode

3. **Activate the environment**
   ```bash
   conda activate mb-vae-dti
   ```

## Quick Start

**Download the complete data folder** including all datasets and pre-trained models:
   - Link: TBA (coming soon)
   - This provides everything needed to immediately start experimenting with the models.

**Note:** This project is currently in development, with the processing section implemented and training/validation components in progress.

## Current Progress

- âœ… Data loading and preprocessing
- ðŸ”„ Embedding generation and processing (in progress)
- ðŸ”„ h5torch file creation and dataset splitting (in progress)
- ðŸ”„ Model training (later)
- ðŸ”„ Model validation and analysis (later)

The training section has not yet been refactored to the new codebase. We are actively working on implementing and testing the quickstart. There currently are still some `archive` folders spread throughout the repository, which contain old code to be refactored.

## Scripts

These scripts are designed to run on an HPC cluster with `Python 3.9`, `PyTorch 2.1.2` and `CUDA 12.1.1`.
They require seperate package-management.

```bash
# Generate embeddings (to be implemented)
bash scripts/embedding.sh

# Pre-training (to be implemented)
bash scripts/pretrain.sh

# Training (to be implemented)
bash scripts/train.sh

# Validation (to be implemented)
bash scripts/validate.sh
```

### Notebooks

...

## Results

The model has been evaluated on standard DTI benchmark datasets:
...

## Citation

If you use this code in your research, please cite:

```
@article{mbvae_dti,
  title={Multi-branch Variational Autoencoders for Drug-target Interaction Prediction and Molecular Generation},
  author={Claeys, Robbe},
  year={2025}
}
```

## Acknowledgments

This work builds upon several open-source projects, including [DiGress](https://github.com/cvignac/DiGress), [DiffMS](https://github.com/coleygroup/DiffMS), and the various embedding models mentioned above.