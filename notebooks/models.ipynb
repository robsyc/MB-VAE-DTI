{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.git', 'utils', 'data', 'bmfm_sm', 'scripts', '.gitignore', 'README.md', 'ESPF', 'notebooks', 'hpc.pbs', 'requirements.txt']\n",
      "/home/robsyc/Desktop/thesis/MB-VAE-DTI\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.chdir('/home/robsyc/Desktop/thesis/MB-VAE-DTI/')\n",
    "print(os.listdir('.'))\n",
    "print(os.getcwd())\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters\n",
      " - Branch 0: 16,573,970\n",
      " - Branch 1: 6,058,514\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m x0 \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m2048\u001b[39m), torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m768\u001b[39m)]\n\u001b[1;32m     35\u001b[0m x1 \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m256\u001b[39m)]\n\u001b[0;32m---> 36\u001b[0m z, kl \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_kl_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m z\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# model = Generator(\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#     latent_dim=777,\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#     hidden_dim=222,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# x = model(z)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# x.shape\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mbvae_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mbvae_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/thesis/MB-VAE-DTI/utils/modelBuilding.py:294\u001b[0m, in \u001b[0;36mMultiBranchDTI.forward\u001b[0;34m(self, x0, x1, compute_kl_loss)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch0(x0) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch1(x1), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    293\u001b[0m z0, kl0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranch0(x0, compute_kl_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 294\u001b[0m z1, kl1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbranch1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_kl_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msum(z0 \u001b[38;5;241m*\u001b[39m z1, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), (kl0 \u001b[38;5;241m+\u001b[39m kl1) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/mbvae_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mbvae_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/thesis/MB-VAE-DTI/utils/modelBuilding.py:73\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[0;34m(self, x, compute_kl_loss)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, compute_kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m        - x: torch.Tensor, the input tensor of shape (batch_size, input_dim)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m        - kl_loss: torch.Tensor, the KL divergence loss\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput2hidden\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layers):\n\u001b[1;32m     75\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norms[i](x \u001b[38;5;241m+\u001b[39m layer(x))\n",
      "File \u001b[0;32m~/miniconda3/envs/mbvae_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mbvae_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mbvae_env/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/mbvae_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mbvae_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mbvae_env/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "from utils.modelBuilding import EncoderBlock, MultiViewBlock, MultiBranchDTI, Generator\n",
    "import torch\n",
    "\n",
    "model = EncoderBlock(\n",
    "    input_dim=2048,\n",
    "    hidden_dim=1024,\n",
    "    output_dim=777,\n",
    "    depth=2,\n",
    "    variational=True\n",
    ")\n",
    "x = torch.randn(10, 2048)\n",
    "z, kl = model(x, compute_kl_loss=True)\n",
    "z.shape\n",
    "\n",
    "model = MultiViewBlock(\n",
    "    input_dim_list=[2048, 768],\n",
    "    hidden_dim=1024,\n",
    "    latent_dim=777,\n",
    "    depth=2,\n",
    "    variational=True\n",
    ")\n",
    "x = [torch.randn(10, 2048), torch.randn(10, 768)]\n",
    "z, kl = model(x, compute_kl_loss=True)\n",
    "z.shape\n",
    "\n",
    "model = MultiBranchDTI(\n",
    "    input_dim_list_0=[2048, 768],\n",
    "    input_dim_list_1=[256],\n",
    "    hidden_dim=1024,\n",
    "    latent_dim=777,\n",
    "    depth=2,\n",
    "    variational=True\n",
    ")\n",
    "x0 = [torch.randn(10, 2048), torch.randn(10, 768)]\n",
    "x1 = torch.randn(10, 256)\n",
    "z, kl = model(x0, x1, compute_kl_loss=True)\n",
    "z.shape\n",
    "\n",
    "# model = Generator(\n",
    "#     latent_dim=777,\n",
    "#     hidden_dim=222,\n",
    "#     depth=4,\n",
    "#     dropout_prob=0.1,\n",
    "#     n_nodes=60,\n",
    "#     n_iters=3\n",
    "# )\n",
    "# z = torch.randn(10, 777)\n",
    "# x = model(z)\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (latent2hidden): Linear(in_features=777, out_features=222, bias=True)\n",
       "  (residual_blocks): ModuleList(\n",
       "    (0-3): 4 x Sequential(\n",
       "      (0): LayerNorm((222,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=222, out_features=222, bias=False)\n",
       "      (2): ELU(alpha=1.0)\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "      (4): Linear(in_features=222, out_features=222, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (hidden2topology): Linear(in_features=222, out_features=3540, bias=True)\n",
       "  (gumbel2hidden): Sequential(\n",
       "    (0): Linear(in_features=1770, out_features=222, bias=True)\n",
       "    (1): ELU(alpha=1.0)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VariationalMultiBranch(\n",
       "  (branch0): Branch(\n",
       "    (encoders): ModuleList(\n",
       "      (0): Encoder(\n",
       "        (input2hidden): Sequential(\n",
       "          (0): Linear(in_features=2048, out_features=128, bias=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (hidden_layers): ModuleList(\n",
       "          (0-1): 2 x Sequential(\n",
       "            (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (layer_norms): ModuleList(\n",
       "          (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (hidden2output): Linear(in_features=128, out_features=1024, bias=True)\n",
       "      )\n",
       "      (1): Encoder(\n",
       "        (input2hidden): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=128, bias=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (hidden_layers): ModuleList(\n",
       "          (0-1): 2 x Sequential(\n",
       "            (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (1): SiLU()\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (layer_norms): ModuleList(\n",
       "          (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (hidden2output): Linear(in_features=128, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (to_attn_logits): Conv1d(1024, 1, kernel_size=(1,), stride=(1,))\n",
       "  )\n",
       "  (branch1): Encoder(\n",
       "    (input2hidden): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "      (1): SiLU()\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (hidden_layers): ModuleList(\n",
       "      (0-1): 2 x Sequential(\n",
       "        (0): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (layer_norms): ModuleList(\n",
       "      (0-1): 2 x LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (hidden2output): Linear(in_features=128, out_features=1024, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9148, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PlainBranch(\n",
       "  (encoders): ModuleList(\n",
       "    (0): Encoder(\n",
       "      (input2hidden): Sequential(\n",
       "        (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (hidden_layers): ModuleList(\n",
       "        (0-1): 2 x Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norms): ModuleList(\n",
       "        (0-1): 2 x LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (hidden2output): Linear(in_features=1024, out_features=500, bias=True)\n",
       "    )\n",
       "    (1): Encoder(\n",
       "      (input2hidden): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=1024, bias=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (hidden_layers): ModuleList(\n",
       "        (0-1): 2 x Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (layer_norms): ModuleList(\n",
       "        (0-1): 2 x LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (hidden2output): Linear(in_features=1024, out_features=500, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (to_attn_logits): Conv1d(500, 1, kernel_size=(1,), stride=(1,))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VariationalMultiBranch(\n",
       "  (branch0): VariationalBranch(\n",
       "    (blocks): ModuleList(\n",
       "      (0): VariationalEncoder(\n",
       "        (input2hidden): Sequential(\n",
       "          (0): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (2): SiLU()\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (hidden_layers): ModuleList(\n",
       "          (0-1): 2 x Sequential(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): SiLU()\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (hidden2output): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "      (1): VariationalEncoder(\n",
       "        (input2hidden): Sequential(\n",
       "          (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=768, out_features=512, bias=True)\n",
       "          (2): SiLU()\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (hidden_layers): ModuleList(\n",
       "          (0-1): 2 x Sequential(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): SiLU()\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (hidden2output): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (aggregator): AttentiveAggregator(\n",
       "      (project_to_samespace): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "      (to_attn_logits): Conv1d(256, 1, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (branch1): VariationalBranch(\n",
       "    (blocks): ModuleList(\n",
       "      (0): VariationalEncoder(\n",
       "        (input2hidden): Sequential(\n",
       "          (0): LayerNorm((715,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=715, out_features=512, bias=True)\n",
       "          (2): SiLU()\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (hidden_layers): ModuleList(\n",
       "          (0-1): 2 x Sequential(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): SiLU()\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (hidden2output): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "      (1-2): 2 x VariationalEncoder(\n",
       "        (input2hidden): Sequential(\n",
       "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (2): SiLU()\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (hidden_layers): ModuleList(\n",
       "          (0-1): 2 x Sequential(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (2): SiLU()\n",
       "            (3): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (hidden2output): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (aggregator): AttentiveAggregator(\n",
       "      (project_to_samespace): ModuleList(\n",
       "        (0-2): 3 x Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "      (to_attn_logits): Conv1d(256, 1, kernel_size=(1,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'central': tensor([5.0000, 5.0000, 5.4437, 5.0000, 5.0000, 5.0000, 5.0000, 5.0000, 5.0000,\n",
       "         5.0000, 5.0000, 5.0000]),\n",
       " '0/Drug_ID': ['9926054',\n",
       "  '9926054',\n",
       "  '216239',\n",
       "  '11717001',\n",
       "  '216239',\n",
       "  '123631',\n",
       "  '123631',\n",
       "  '9926791',\n",
       "  '156422',\n",
       "  '216239',\n",
       "  '9926054',\n",
       "  '9926791'],\n",
       " '0/Drug_SMILES': ['Cc1ccc2nc(NCCN)c3ncc(C)n3c2c1.Cl',\n",
       "  'Cc1ccc2nc(NCCN)c3ncc(C)n3c2c1.Cl',\n",
       "  'CNC(=O)c1cc(Oc2ccc(NC(=O)Nc3ccc(Cl)c(C(F)(F)F)c3)cc2)ccn1',\n",
       "  'OCCn1cc(-c2ccc3c(c2)CCC3=NO)c(-c2ccncc2)n1',\n",
       "  'CNC(=O)c1cc(Oc2ccc(NC(=O)Nc3ccc(Cl)c(C(F)(F)F)c3)cc2)ccn1',\n",
       "  'COc1cc2ncnc(Nc3ccc(F)c(Cl)c3)c2cc1OCCCN1CCOCC1',\n",
       "  'COc1cc2ncnc(Nc3ccc(F)c(Cl)c3)c2cc1OCCCN1CCOCC1',\n",
       "  'CC1CCN(C(=O)CC#N)CC1N(C)c1ncnc2[nH]ccc12',\n",
       "  'Cc1ccc(-n2nc(C(C)(C)C)cc2NC(=O)Nc2ccc(OCCN3CCOCC3)c3ccccc23)cc1',\n",
       "  'CNC(=O)c1cc(Oc2ccc(NC(=O)Nc3ccc(Cl)c(C(F)(F)F)c3)cc2)ccn1',\n",
       "  'Cc1ccc2nc(NCCN)c3ncc(C)n3c2c1.Cl',\n",
       "  'CC1CCN(C(=O)CC#N)CC1N(C)c1ncnc2[nH]ccc12'],\n",
       " '0/Drug_emb_graph': tensor([[-0.0104, -0.0124, -0.0965,  ..., -0.0641, -0.0203, -0.0963],\n",
       "         [-0.0104, -0.0124, -0.0965,  ..., -0.0641, -0.0203, -0.0963],\n",
       "         [ 0.0428,  0.0758, -0.0609,  ..., -0.0239,  0.0405, -0.0535],\n",
       "         ...,\n",
       "         [ 0.0428,  0.0758, -0.0609,  ..., -0.0239,  0.0405, -0.0535],\n",
       "         [-0.0104, -0.0124, -0.0965,  ..., -0.0641, -0.0203, -0.0963],\n",
       "         [ 0.0389,  0.0694, -0.0635,  ..., -0.0269,  0.0361, -0.0566]]),\n",
       " '0/Drug_emb_image': tensor([[0.8298, 0.4099, 0.8377,  ..., 0.4596, 0.5890, 0.8798],\n",
       "         [0.8298, 0.4099, 0.8377,  ..., 0.4596, 0.5890, 0.8798],\n",
       "         [0.8339, 0.4089, 0.8461,  ..., 0.4614, 0.5940, 0.8814],\n",
       "         ...,\n",
       "         [0.8339, 0.4089, 0.8461,  ..., 0.4614, 0.5940, 0.8814],\n",
       "         [0.8298, 0.4099, 0.8377,  ..., 0.4596, 0.5890, 0.8798],\n",
       "         [0.8343, 0.4086, 0.8492,  ..., 0.4632, 0.5932, 0.8809]]),\n",
       " '0/Drug_emb_text': tensor([[ 0.7918, -0.1962,  0.6254,  ...,  1.5386,  0.4505, -0.4826],\n",
       "         [ 0.7918, -0.1962,  0.6254,  ...,  1.5386,  0.4505, -0.4826],\n",
       "         [ 1.2933, -0.5791,  0.3090,  ...,  0.7474,  0.2293, -0.3161],\n",
       "         ...,\n",
       "         [ 1.2933, -0.5791,  0.3090,  ...,  0.7474,  0.2293, -0.3161],\n",
       "         [ 0.7918, -0.1962,  0.6254,  ...,  1.5386,  0.4505, -0.4826],\n",
       "         [ 1.2840, -0.1021,  0.8307,  ...,  1.6814,  0.8176, -0.7826]]),\n",
       " '0/Drug_fp': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " '1/Target_ID': ['CASK',\n",
       "  'AKT2',\n",
       "  'MERTK',\n",
       "  'ULK3',\n",
       "  'MARK1',\n",
       "  'NEK5',\n",
       "  'TYRO3',\n",
       "  'MKK7',\n",
       "  'EIF2AK1',\n",
       "  'PFPK5(Pfalciparum)',\n",
       "  'TBK1',\n",
       "  'TGFBR1'],\n",
       " '1/Target_emb_DNA': tensor([[ 0.0296,  0.1325, -0.1994,  ..., -0.0774,  0.2888,  0.0753],\n",
       "         [ 0.3043,  0.0600, -0.3898,  ...,  0.0360,  0.3852,  0.1363],\n",
       "         [-0.0472,  0.1073, -0.2165,  ..., -0.1279,  0.3177,  0.0547],\n",
       "         ...,\n",
       "         [ 0.0757,  0.1019,  0.0509,  ..., -0.0800,  0.1791,  0.1868],\n",
       "         [ 0.0293,  0.1434, -0.1350,  ..., -0.0732,  0.3404,  0.0617],\n",
       "         [-0.0098,  0.1211, -0.0737,  ..., -0.0226,  0.4175,  0.0341]]),\n",
       " '1/Target_emb_ESM': tensor([[ 0.0086,  0.0184, -0.0187,  ..., -0.1159,  0.0551,  0.1150],\n",
       "         [ 0.0082, -0.0627, -0.0160,  ..., -0.1736,  0.1109,  0.1688],\n",
       "         [ 0.0078, -0.0503, -0.0244,  ..., -0.1274,  0.0460,  0.1340],\n",
       "         ...,\n",
       "         [ 0.0194, -0.0007, -0.0113,  ..., -0.1334,  0.0747,  0.0167],\n",
       "         [ 0.0144, -0.0483,  0.0067,  ..., -0.1049,  0.0697,  0.0873],\n",
       "         [-0.0281, -0.0432, -0.0271,  ..., -0.1195,  0.0528,  0.0182]]),\n",
       " '1/Target_emb_T5': tensor([[-0.0367,  0.0011, -0.0247,  ...,  0.0115, -0.0380,  0.0008],\n",
       "         [-0.0109,  0.0202, -0.0607,  ...,  0.0052, -0.0405, -0.0845],\n",
       "         [-0.0480,  0.0243, -0.0127,  ..., -0.0373, -0.0313, -0.0020],\n",
       "         ...,\n",
       "         [-0.0084,  0.0031, -0.0418,  ..., -0.0203, -0.0386, -0.0889],\n",
       "         [-0.0422, -0.0085, -0.0109,  ..., -0.0384, -0.0102, -0.0364],\n",
       "         [-0.0242,  0.0233, -0.0262,  ..., -0.0082, -0.0426, -0.0080]]),\n",
       " '1/Target_fp': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 1., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " '1/Target_seq': ['MADDDVLFEDVYELCEVIGKGPFSVVRRCINRETGQQFAVKIVDVAKFTSSPGLSTEDLKREASICHMLKHPHIVELLETYSSDGMLYMVFEFMDGADLCFEIVKRADAGFVYSEAVASHYMRQILEALRYCHDNNIIHRDVKPHCVLLASKENSAPVKLGGFGVAIQLGESGLVAGGRVGTPHFMAPEVVKREPYGKPVDVWGCGVILFILLSGCLPFYGTKERLFEGIIKGKYKMNPRQWSHISESAKDLVRRMLMLDPAERITVYEALNHPWLKERDRYAYKIHLPETVEQLRKFNARRKLKGAVLAAVSSHKFNSFYGDPPEELPDFSEDPTSSGLLAAERAVSQVLDSLEEIHALTDCSEKDLDFLHSVFQDQHLHTLLDLYDKINTKSSPQIRNPPSDAVQRAKEVLEEISCYPENNDAKELKRILTQPHFMALLQTHDVVAHEVYSDEALRVTPPPTSPYLNGDSPESANGDMDMENVTRVRLVQFQKNTDEPMGITLKMNELNHCIVARIMHGGMIHRQGTLHVGDEIREINGISVANQTVEQLQKMLREMRGSITFKIVPSYRTQSSSCERDSPSTSRQSPANGHSSTNNSVSDLPSTTQPKGRQIYVRAQFEYDPAKDDLIPCKEAGIRFRVGDIIQIISKDDHNWWQGKLENSKNGTAGLIPSPELQEWRVACIAMEKTKQEQQASCTWFGKKKKQYKDKYLAKHNAVFDQLDLVTYEEVVKLPAFKRKTLVLLGAHGVGRRHIKNTLITKHPDRFAYPIPHTTRPPKKDEENGKNYYFVSHDQMMQDISNNEYLEYGSHEDAMYGTKLETIRKIHEQGLIAILDVEPQALKVLRTAEFAPFVVFIAAPTITPGLNEDESLQRLQKESDILQRTYAHYFDLTIINNEIDETIRHLEEAVELVCTAPQWVPVSWVY',\n",
       "  'MNEVSVIKEGWLHKRGEYIKTWRPRYFLLKSDGSFIGYKERPEAPDQTLPPLNNFSVAECQLMKTERPRPNTFVIRCLQWTTVIERTFHVDSPDEREEWMRAIQMVANSLKQRAPGEDPMDYKCGSPSDSSTTEEMEVAVSKARAKVTMNDFDYLKLLGKGTFGKVILVREKATGRYYAMKILRKEVIIAKDEVAHTVTESRVLQNTRHPFLTALKYAFQTHDRLCFVMEYANGGELFFHLSRERVFTEERARFYGAEIVSALEYLHSRDVVYRDIKLENLMLDKDGHIKITDFGLCKEGISDGATMKTFCGTPEYLAPEVLEDNDYGRAVDWWGLGVVMYEMMCGRLPFYNQDHERLFELILMEEIRFPRTLSPEAKSLLAGLLKKDPKQRLGGGPSDAKEVMEHRFFLSINWQDVVQKKLLPPFKPQVTSEVDTRYFDDEFTAQSITITPPDRYDSLGLLELDQRTHFPQFSYSASIRE',\n",
       "  'MGPAPLPLLLGLFLPALWRRAITEAREEAKPYPLFPGPFPGSLQTDHTPLLSLPHASGYQPALMFSPTQPGRPHTGNVAIPQVTSVESKPLPPLAFKHTVGHIILSEHKGVKFNCSISVPNIYQDTTISWWKDGKELLGAHHAITQFYPDDEVTAIIASFSITSVQRSDNGSYICKMKINNEEIVSDPIYIEVQGLPHFTKQPESMNVTRNTAFNLTCQAVGPPEPVNIFWVQNSSRVNEQPEKSPSVLTVPGLTEMAVFSCEAHNDKGLTVSKGVQINIKAIPSPPTEVSIRNSTAHSILISWVPGFDGYSPFRNCSIQVKEADPLSNGSVMIFNTSALPHLYQIKQLQALANYSIGVSCMNEIGWSAVSPWILASTTEGAPSVAPLNVTVFLNESSDNVDIRWMKPPTKQQDGELVGYRISHVWQSAGISKELLEEVGQNGSRARISVQVHNATCTVRIAAVTRGGVGPFSDPVKIFIPAHGWVDYAPSSTPAPGNADPVLIIFGCFCGFILIGLILYISLAIRKRVQETKFGNAFTEEDSELVVNYIAKKSFCRRAIELTLHSLGVSEELQNKLEDVVIDRNLLILGKILGEGEFGSVMEGNLKQEDGTSLKVAVKTMKLDNSSQREIEEFLSEAACMKDFSHPNVIRLLGVCIEMSSQGIPKPMVILPFMKYGDLHTYLLYSRLETGPKHIPLQTLLKFMVDIALGMEYLSNRNFLHRDLAARNCMLRDDMTVCVADFGLSKKIYSGDYYRQGRIAKMPVKWIAIESLADRVYTSKSDVWAFGVTMWEIATRGMTPYPGVQNHEMYDYLLHGHRLKQPEDCLDELYEIMYSCWRTDPLDRPTFSVLRLQLEKLLESLPDVRNQADVIYVNTQLLESSEGLAQGSTLAPLDLNIDPDSIIASCTPRAAISVVTAEVHDSKPHEGRYILNGGSEEWEDLTSAPSAAVTAEKNSVLPGERLVRNGVSWSHSSMLPLGSSLPDELLFADDSSEGSEVLM',\n",
       "  'MAGPGWGPPRLDGFILTERLGSGTYATVYKAYAKKDTREVVAIKCVAKKSLNKASVENLLTEIEILKGIRHPHIVQLKDFQWDSDNIYLIMEFCAGGDLSRFIHTRRILPEKVARVFMQQLASALQFLHERNISHLDLKPQNILLSSLEKPHLKLADFGFAQHMSPWDEKHVLRGSPLYMAPEMVCQRQYDARVDLWSMGVILYEALFGQPPFASRSFSELEEKIRSNRVIELPLRPLLSRDCRDLLQRLLERDPSRRISFQDFFAHPWVDLEHMPSGESLGRATALVVQAVKKDQEGDSAAALSLYCKALDFFVPALHYEVDAQRKEAIKAKVGQYVSRAEELKAIVSSSNQALLRQGTSARDLLREMARDKPRLLAALEVASAAMAKEEAAGGEQDALDLYQHSLGELLLLLAAEPPGRRRELLHTEVQNLMARAEYLKEQVKMRESRWEADTLDKEGLSESVRSSCTLQ',\n",
       "  'MSARTPLPTVNERDTENHTSVDGYTEPHIQPTKSSSRQNIPRCRNSITSATDEQPHIGNYRLQKTIGKGNFAKVKLARHVLTGREVAVKIIDKTQLNPTSLQKLFREVRIMKILNHPNIVKLFEVIETEKTLYLVMEYASGGEVFDYLVAHGRMKEKEARAKFRQIVSAVQYCHQKYIVHRDLKAENLLLDGDMNIKIADFGFSNEFTVGNKLDTFCGSPPYAAPELFQGKKYDGPEVDVWSLGVILYTLVSGSLPFDGQNLKELRERVLRGKYRIPFYMSTDCENLLKKLLVLNPIKRGSLEQIMKDRWMNVGHEEEELKPYTEPDPDFNDTKRIDIMVTMGFARDEINDALINQKYDEVMATYILLGRKPPEFEGGESLSSGNLCQRSRPSSDLNNSTLQSPAHLKVQRSISANQKQRRFSDHAGPSIPPAVSYTKRPQANSVESEQKEEWDKDVARKLGSTTVGSKSEMTASPLVGPERKKSSTIPSNNVYSGGSMARRNTYVCERTTDRYVALQNGKDSSLTEMSVSSISSAGSSVASAVPSARPRHQKSMSTSGHPIKVTLPTIKDGSEAYRPGTTQRVPAASPSAHSISTATPDRTRFPRGSSSRSTFHGEQLRERRSVAYNGPPASPSHETGAFAHARRGTSTGIISKITSKFVRRDPSEGEASGRTDTSRSTSGEPKERDKEEGKDSKPRSLRFTWSMKTTSSMDPNDMMREIRKVLDANNCDYEQKERFLLFCVHGDARQDSLVQWEMEVCKLPRLSLNGVRFKRISGTSIAFKNIASKIANELKL',\n",
       "  'MDKYDVIKAIGQGAFGKAYLAKGKSDSKHCVIKEINFEKMPIQEKEASKKEVILLEKMKHPNIVAFFNSFQENGRLFIVMEYCDGGDLMKRINRQRGVLFSEDQILGWFVQISLGLKHIHDRKILHRDIKAQNIFLSKNGMVAKLGDFGIARVLNNSMELARTCIGTPYYLSPEICQNKPYNNKTDIWSLGCVLYELCTLKHPFEGNNLQQLVLKICQAHFAPISPGFSRELHSLISQLFQVSPRDRPSINSILKRPFLENLIPKYLTPEVIQEEFSHMLICRAGAPASRHAGKVVQKCKIQKVRFQGKCPPRSRISVPIKRNAILHRNEWRPPAGAQKARSIKMIERPKIAAVCGHYDYYYAQLDMLRRRAHKPSYHPIPQENTGVEDYGQETRHGPSPSQWPAEYLQRKFEAQQYKLKVEKQLGLRPSSAEPNYNQRQELRSNGEEPRFQELPFRKNEMKEQEYWKQLEEIRQQYHNDMKEIRKKMGREPEENSKISHKTYLVKKSNLPVHQDASEGEAPVQMEFRSCCPGWSAMARSWLTATSASQDIEKDLKQMRLQNTKESKNPEQKYKAKKGVKFEINLDKCISDENILQEEEAMDIPNETLTFEDGMKFKEYECVKEHGDYTDKAFEKLHCPEAGFSTQTVAAVGNRRQWDGGAPQTLLQMMAVADITSTCPTGPDSESVLSVSRQEGKTKDPYSPVLILM',\n",
       "  'MALRRSMGRPGLPPLPLPPPPRLGLLLAALASLLLPESAAAGLKLMGAPVKLTVSQGQPVKLNCSVEGMEEPDIQWVKDGAVVQNLDQLYIPVSEQHWIGFLSLKSVERSDAGRYWCQVEDGGETEISQPVWLTVEGVPFFTVEPKDLAVPPNAPFQLSCEAVGPPEPVTIVWWRGTTKIGGPAPSPSVLNVTGVTQSTMFSCEAHNLKGLASSRTATVHLQALPAAPFNITVTKLSSSNASVAWMPGADGRALLQSCTVQVTQAPGGWEVLAVVVPVPPFTCLLRDLVPATNYSLRVRCANALGPSPYADWVPFQTKGLAPASAPQNLHAIRTDSGLILEWEEVIPEAPLEGPLGPYKLSWVQDNGTQDELTVEGTRANLTGWDPQKDLIVRVCVSNAVGCGPWSQPLVVSSHDRAGQQGPPHSRTSWVPVVLGVLTALVTAAALALILLRKRRKETRFGQAFDSVMARGEPAVHFRAARSFNRERPERIEATLDSLGISDELKEKLEDVLIPEQQFTLGRMLGKGEFGSVREAQLKQEDGSFVKVAVKMLKADIIASSDIEEFLREAACMKEFDHPHVAKLVGVSLRSRAKGRLPIPMVILPFMKHGDLHAFLLASRIGENPFNLPLQTLIRFMVDIACGMEYLSSRNFIHRDLAARNCMLAEDMTVCVADFGLSRKIYSGDYYRQGCASKLPVKWLALESLADNLYTVQSDVWAFGVTMWEIMTRGQTPYAGIENAEIYNYLIGGNRLKQPPECMEDVYDLMYQCWSADPKQRPSFTCLRMELENILGQLSVLSASQDPLYINIERAEEPTAGGSLELPGRDQPYSGAGDGSGMGAVGGTPSDCRYILTPGGLAEQPGQAEHQPESPLNETQRLLLLQQGLLPHSSC',\n",
       "  'MAASSLEQKLSRLEAKLKQENREARRRIDLNLDISPQRPRPTLQLPLANDGGSRSPSSESSPQHPTPPARPRHMLGLPSTLFTPRSMESIEIDQKLQEIMKQTGYLTIGGQRYQAEINDLENLGEMGSGTCGQVWKMRFRKTGHVIAVKQMRRSGNKEENKRILMDLDVVLKSHDCPYIVQCFGTFITNTDVFIAMELMGTCAEKLKKRMQGPIPERILGKMTVAIVKALYYLKEKHGVIHRDVKPSNILLDERGQIKLCDFGISGRLVDSKAKTRSAGCAAYMAPERIDPPDPTKPDYDIRADVWSLGISLVELATGQFPYKNCKTDFEVLTKVLQEEPPLLPGHMGFSGDFQSFVKDCLTKDHRKRPKYNKLLEHSFIKRYETLEVDVASWFKDVMAKTESPRTSGVLSQPHLPFFR',\n",
       "  'MQGGNSGVRKREEEGDGAGAVAAPPAIDFPAEGPDPEYDESDVPAEIQVLKEPLQQPTFPFAVANQLLLVSLLEHLSHVHEPNPLRSRQVFKLLCQTFIKMGLLSSFTCSDEFSSLRLHHNRAITHLMRSAKERVRQDPCEDISRIQKIRSREVALEAQTSRYLNEFEELAILGKGGYGRVYKVRNKLDGQYYAIKKILIKGATKTVCMKVLREVKVLAGLQHPNIVGYHTAWIEHVHVIQPRADRAAIELPSLEVLSDQEEDREQCGVKNDESSSSSIIFAEPTPEKEKRFGESDTENQNNKSVKYTTNLVIRESGELESTLELQENGLAGLSASSIVEQQLPLRRNSHLEESFTSTEESSEENVNFLGQTEAQYHLMLHIQMQLCELSLWDWIVERNKRGREYVDESACPYVMANVATKIFQELVEGVFYIHNMGIVHRDLKPRNIFLHGPDQQVKIGDFGLACTDILQKNTDWTNRNGKRTPTHTSRVGTCLYASPEQLEGSEYDAKSDMYSLGVVLLELFQPFGTEMERAEVLTGLRTGQLPESLRKRCPVQAKYIQHLTRRNSSQRPSAIQLLQSELFQNSGNVNLTLQMKIIEQEKEIAELKKQLNLLSQDKGVRDDGKDGGVG',\n",
       "  'MEKYHGLEKIGEGTYGVVYKAQNNYGETFALKKIRLEKEDEGIPSTTIREISILKELKHSNIVKLYDVIHTKKRLVLVFEHLDQDLKKLLDVCEGGLESVTAKSFLLQLLNGIAYCHDRRVLHRDLKPQNLLINREGELKIADFGLARAFGIPVRKYTHEVVTLWYRAPDVLMGSKKYSTTIDIWSVGCIFAEMVNGTPLFPGVSEADQLMRIFRILGTPNSKNWPNVTELPKYDPNFTVYEPLPWESFLKGLDESGIDLLSKMLKLDPNQRITAKQALEHAYFKENN',\n",
       "  'MQSTSNHLWLLSDILGQGATANVFRGRHKKTGDLFAIKVFNNISFLRPVDVQMREFEVLKKLNHKNIVKLFAIEEETTTRHKVLIMEFCPCGSLYTVLEEPSNAYGLPESEFLIVLRDVVGGMNHLRENGIVHRDIKPGNIMRVIGEDGQSVYKLTDFGAARELEDDEQFVSLYGTEEYLHPDMYERAVLRKDHQKKYGATVDLWSIGVTFYHAATGSLPFRPFEGPRRNKEVMYKIITGKPSGAISGVQKAENGPIDWSGDMPVSCSLSRGLQVLLTPVLANILEADQEKCWGFDQFFAETSDILHRMVIHVFSLQQMTAHKIYIHSYNTATIFHELVYKQTKIISSNQELIYEGRRLVLEPGRLAQHFPKTTEENPIFVVSREPLNTIGLIYEKISLPKVHPRYDLDGDASMAKAITGVVCYACRIASTLLLYQELMRKGIRWLIELIKDDYNETVHKKTEVVITLDFCIRNIEKTVKVYEKLMKINLEAAELGEISDIHTKLLRLSSSQGTIETSLQDIDSRLSPGGSLADAWAHQEGTHPKDRNVEKLQVLLNCMTEIYYQFKKDKAERRLAYNEEQIHKFDKQKLYYHATKAMTHFTDECVKKYEAFLNKSEEWIRKMLHLRKQLLSLTNQCFDIEEEVSKYQEYTNELQETLPQKMFTASSGIKHTMTPIYPSSNTLVEMTLGMKKLKEEMEGVVKELAENNHILERFGSLTMDGGLRNVDCL',\n",
       "  'MEAAVAAPRPRLLLLVLAAAAAAAAALLPGATALQCFCHLCTKDNFTCVTDGLCFVSVTETTDKVIHNSMCIAEIDLIPRDRPFVCAPSSKTGSVTTTYCCNQDHCNKIELPTTVKSSPGLGPVELAAVIAGPVCFVCISLMLMVYICHNRTVIHHRVPNEEDPSLDRPFISEGTTLKDLIYDMTTSGSGSGLPLLVQRTIARTIVLQESIGKGRFGEVWRGKWRGEEVAVKIFSSREERSWFREAEIYQTVMLRHENILGFIAADNKDNGTWTQLWLVSDYHEHGSLFDYLNRYTVTVEGMIKLALSTASGLAHLHMEIVGTQGKPAIAHRDLKSKNILVKKNGTCCIADLGLAVRHDSATDTIDIAPNHRVGTKRYMAPEVLDDSINMKHFESFKRADIYAMGLVFWEIARRCSIGGIHEDYQLPYYDLVPSDPSVEEMRKVVCEQKLRPNIPNRWQSCEALRVMAKIMRECWYANGAARLTALRIKKTLSQLSQQEGIKM'],\n",
       " '1/Target_seq_DNA': ['ATGGCCGACGACGACGTGCTGTTCGAGGATGTGTACGAGCTGTGCGAGGTGATCGGAAAGGGTCCCTTCAGTGTTGTACGACGATGTATCAACAGAGAAACTGGGCAACAATTTGCTGTAAAAATTGTTGATGTAGCCAAGTTCACATCAAGTCCAGGGTTAAGTACAGAAGATCTAAAGCGGGAAGCCAGTATCTGTCATATGCTGAAACATCCACACATTGTAGAGTTATTGGAGACATATAGCTCAGATGGAATGCTTTACATGGTTTTCGAATTTATGGATGGAGCAGATCTGTGTTTTGAAATCGTAAAGCGAGCTGACGCTGGTTTTGTGTACAGTGAAGCTGTAGCCAGCCATTATATGAGACAGATACTGGAAGCTCTACGCTACTGCCATGATAATAACATAATTCACAGGGATGTGAAGCCCCACTGTGTTCTCCTTGCCTCAAAAGAAAACTCGGCACCTGTTAAACTTGGAGGCTTTGGGGTAGCTATTCAATTAGGGGAGTCTGGACTTGTAGCTGGAGGACGTGTTGGAACACCTCATTTTATGGCACCAGAAGTGGTCAAAAGAGAGCCTTACGGAAAGCCTGTAGACGTCTGGGGGTGCGGTGTGATCCTTTTTATCCTGCTCAGTGGTTGTTTGCCTTTTTACGGAACCAAGGAAAGATTGTTTGAAGGCATTATTAAAGGAAAATATAAGATGAATCCAAGGCAGTGGAGCCATATCTCTGAAAGTGCCAAAGACCTAGTACGTCGCATGCTGATGCTGGATCCAGCTGAAAGGATCACTGTTTATGAAGCACTGAATCACCCATGGCTTAAGGAGCGGGATCGTTACGCCTACAAGATTCATCTTCCAGAAACAGTAGAGCAGCTGAGGAAATTCAATGCAAGGAGGAAACTAAAGGGTGCAGTACTAGCCGCTGTGTCAAGTCACAAATTCAACTCATTCTATGGGGATCCCCCTGAAGAGTTACCAGATTTCTCCGAAGACCCTACCTCCTCAGGACTTCTAGCAGCAGAAAGAGCAGTCTCACAGGTGCTGGACAGCCTGGAAGAGATTCATGCGCTTACAGACTGCAGTGAAAAGGACCTAGATTTTCTACACAGTGTTTTCCAGGATCAGCATCTTCACACACTACTAGATCTGTATGACAAAATTAACACAAAGTCTTCACCACAAATCAGGAATCCTCCAAGCGATGCAGTACAGAGAGCCAAAGAGGTATTGGAAGAAATTTCATGTTACCCTGAGAATAACGACGCAAAGGAACTAAAGCGTATTTTAACACAACCTCATTTCATGGCCTTACTTCAGACTCACGACGTAGTGGCACATGAAGTTTACAGTGATGAAGCATTGAGGGTCACACCTCCTCCCACCTCTCCCTATTTAAACGGCGATTCTCCAGAAAGTGCTAACGGAGACATGGATATGGAGAATGTGACCAGAGTTCGGCTGGTACAGTTTCAAAAGAACACAGATGAACCAATGGGAATCACTTTAAAAATGAATGAACTAAATCATTGTATTGTTGCAAGAATTATGCATGGGGGCATGATTCACAGGCAAGGTACACTTCATGTTGGTGATGAAATTCGAGAAATCAATGGCATCAGTGTGGCTAACCAAACAGTGGAACAACTGCAAAAAATGCTTAGGGAAATGCGGGGGAGTATTACCTTCAAGATTGTGCCAAGTTACCGCACTCAGTCTTCGTCCTGTGAGAGAGATTCCCCTTCCACTTCCAGACAGTCCCCAGCTAATGGTCATAGCAGCACTAACAATTCTGTTTCGGACTTGCCATCAACTACCCAACCAAAAGGACGACAGATCTATGTAAGAGCACAATTTGAATATGATCCAGCCAAGGATGACCTCATCCCCTGTAAAGAAGCTGGCATTCGATTCAGAGTTGGTGACATCATCCAGATTATTAGTAAGGATGATCATAATTGGTGGCAGGGTAAACTGGAAAACTCCAAAAATGGAACTGCAGGTCTCATTCCTTCTCCTGAACTTCAGGAATGGCGAGTAGCTTGCATTGCCATGGAGAAGACCAAACAGGAGCAGCAGGCCAGCTGTACTTGGTTTGGCAAGAAAAAGAAGCAGTACAAAGATAAATATTTGGCAAAGCACAATGCAGTGTTTGATCAATTAGATCTTGTCACATATGAAGAAGTAGTAAAACTGCCAGCATTCAAGAGGAAAACACTAGTCTTATTAGGCGCACATGGTGTTGGGAGAAGACACATAAAAAACACTCTCATCACAAAGCACCCAGACCGGTTTGCGTACCCTATTCCACATACAACCAGACCTCCAAAGAAAGACGAAGAAAATGGAAAGAATTATTACTTTGTATCTCATGACCAAATGATGCAAGACATCTCTAATAACGAGTACTTGGAGTACGGCAGCCACGAGGATGCGATGTATGGGACAAAACTGGAGACCATCCGGAAGATCCACGAGCAGGGGCTGATTGCAATACTGGACGTGGAGCCTCAGGCACTGAAGGTCCTGAGAACTGCAGAGTTTGCTCCTTTTGTTGTTTTCATTGCTGCACCAACTATTACTCCAGGTTTAAATGAGGATGAATCTCTTCAGCGTCTGCAGAAGGAGTCTGACATCTTACAGAGAACATATGCACACTACTTCGATCTCACAATTATCAACAATGAAATTGATGAGACAATCAGACATCTGGAGGAAGCTGTTGAGCTCGTGTGCACAGCCCCACAGTGGGTCCCTGTCTCCTGGGTCTATTAG',\n",
       "  'ATGAATGAGGTGTCTGTCATCAAAGAAGGCTGGCTCCACAAGCGTGGTGAATACATCAAGACCTGGAGGCCACGGTACTTCCTGCTGAAGAGCGACGGCTCCTTCATTGGGTACAAGGAGAGGCCCGAGGCCCCTGATCAGACTCTACCCCCCTTAAACAACTTCTCCGTAGCAGAATGCCAGCTGATGAAGACCGAGAGGCCGCGACCCAACACCTTTGTCATACGCTGCCTGCAGTGGACCACAGTCATCGAGAGGACCTTCCACGTGGATTCTCCAGACGAGAGGGAGGAGTGGATGCGGGCCATCCAGATGGTCGCCAACAGCCTCAAGCAGCGGGCCCCAGGCGAGGACCCCATGGACTACAAGTGTGGCTCCCCCAGTGACTCCTCCACGACTGAGGAGATGGAAGTGGCGGTCAGCAAGGCACGGGCTAAAGTGACCATGAATGACTTCGACTATCTCAAACTCCTTGGCAAGGGAACCTTTGGCAAAGTCATCCTGGTGCGGGAGAAGGCCACTGGCCGCTACTACGCCATGAAGATCCTGCGGAAGGAAGTCATCATTGCCAAGGATGAAGTCGCTCACACAGTCACCGAGAGCCGGGTCCTCCAGAACACCAGGCACCCGTTCCTCACTGCGCTGAAGTATGCCTTCCAGACCCACGACCGCCTGTGCTTTGTGATGGAGTATGCCAACGGGGGTGAGCTGTTCTTCCACCTGTCCCGGGAGCGTGTCTTCACAGAGGAGCGGGCCCGGTTTTATGGTGCAGAGATTGTCTCGGCTCTTGAGTACTTGCACTCGCGGGACGTGGTATACCGCGACATCAAGCTGGAAAACCTCATGCTGGACAAAGATGGCCACATCAAGATCACTGACTTTGGCCTCTGCAAAGAGGGCATCAGTGACGGGGCCACCATGAAAACCTTCTGTGGGACCCCGGAGTACCTGGCGCCTGAGGTGCTGGAGGACAATGACTATGGCCGGGCCGTGGACTGGTGGGGGCTGGGTGTGGTCATGTACGAGATGATGTGCGGCCGCCTGCCCTTCTACAACCAGGACCACGAGCGCCTCTTCGAGCTCATCCTCATGGAAGAGATCCGCTTCCCGCGCACGCTCAGCCCCGAGGCCAAGTCCCTGCTTGCTGGGCTGCTTAAGAAGGACCCCAAGCAGAGGCTTGGTGGGGGGCCCAGCGATGCCAAGGAGGTCATGGAGCACAGGTTCTTCCTCAGCATCAACTGGCAGGACGTGGTCCAGAAGAAGCTCCTGCCACCCTTCAAACCTCAGGTCACGTCCGAGGTCGACACAAGGTACTTCGATGATGAATTTACCGCCCAGTCCATCACAATCACACCCCCTGACCGCTATGACAGCCTGGGCTTACTGGAGCTGGACCAGCGGACCCACTTCCCCCAGTTCTCCTACTCGGCCAGCATCCGCGAGTGA',\n",
       "  'ATGGGGCCGGCCCCGCTGCCGCTGCTGCTGGGCCTCTTCCTCCCCGCGCTCTGGCGTAGAGCTATCACTGAGGCAAGGGAAGAAGCCAAGCCTTACCCGCTATTCCCGGGACCTTTTCCAGGGAGCCTGCAAACTGACCACACACCGCTGTTATCCCTTCCTCACGCCAGTGGGTACCAGCCTGCCTTGATGTTTTCACCAACCCAGCCTGGAAGACCACATACAGGAAACGTAGCCATTCCCCAGGTGACCTCTGTCGAATCAAAGCCCCTACCGCCTCTTGCCTTCAAACACACAGTTGGACACATAATACTTTCTGAACATAAAGGTGTCAAATTTAATTGCTCAATCAGTGTACCTAATATATACCAGGACACCACAATTTCTTGGTGGAAAGATGGGAAGGAATTGCTTGGGGCACATCATGCAATTACACAGTTTTATCCAGATGATGAAGTTACAGCAATAATCGCTTCCTTCAGCATAACCAGTGTGCAGCGTTCAGACAATGGGTCGTATATCTGTAAGATGAAAATAAACAATGAAGAGATCGTGTCTGATCCCATCTACATCGAAGTACAAGGACTTCCTCACTTTACTAAGCAGCCTGAGAGCATGAATGTCACCAGAAACACAGCCTTCAACCTCACCTGTCAGGCTGTGGGCCCGCCTGAGCCCGTCAACATTTTCTGGGTTCAAAACAGTAGCCGTGTTAACGAACAGCCTGAAAAATCCCCCTCCGTGCTAACTGTTCCAGGCCTGACGGAGATGGCGGTCTTCAGTTGTGAGGCCCACAATGACAAAGGGCTGACCGTGTCCAAGGGAGTGCAGATCAACATCAAAGCAATTCCCTCCCCACCAACTGAAGTCAGCATCCGTAACAGCACTGCACACAGCATTCTGATCTCCTGGGTTCCTGGTTTTGATGGATACTCCCCGTTCAGGAATTGCAGCATTCAGGTCAAGGAAGCTGATCCGCTGAGTAATGGCTCAGTCATGATTTTTAACACCTCTGCCTTACCACATCTGTACCAAATCAAGCAGCTGCAAGCCCTGGCTAATTACAGCATTGGTGTTTCCTGCATGAATGAAATAGGCTGGTCTGCAGTGAGCCCTTGGATTCTAGCCAGCACGACTGAAGGAGCCCCATCAGTAGCACCTTTAAATGTCACTGTGTTTCTGAATGAATCTAGTGATAATGTGGACATCAGATGGATGAAGCCTCCGACTAAGCAGCAGGATGGAGAACTGGTGGGCTACCGGATATCCCACGTGTGGCAGAGTGCAGGGATTTCCAAAGAGCTCTTGGAGGAAGTTGGCCAGAATGGCAGCCGAGCTCGGATCTCTGTTCAAGTCCACAATGCTACGTGCACAGTGAGGATTGCAGCCGTCACCAGAGGGGGAGTTGGGCCCTTCAGTGATCCAGTGAAAATATTTATCCCTGCACACGGTTGGGTAGATTATGCCCCCTCTTCAACTCCGGCGCCTGGCAACGCAGATCCTGTGCTCATCATCTTTGGCTGCTTTTGTGGATTTATTTTGATTGGGTTGATTTTATACATCTCCTTGGCCATCAGAAAAAGAGTCCAGGAGACAAAGTTTGGGAATGCATTCACAGAGGAGGATTCTGAATTAGTGGTGAATTATATAGCAAAGAAATCCTTCTGTCGGCGAGCCATTGAACTTACCTTACATAGCTTGGGAGTCAGTGAGGAACTACAAAATAAACTAGAAGATGTTGTGATTGACAGGAATCTTCTAATTCTTGGAAAAATTCTGGGTGAAGGAGAGTTTGGGTCTGTAATGGAAGGAAATCTTAAGCAGGAAGATGGGACCTCTCTGAAAGTGGCAGTGAAGACCATGAAGTTGGACAACTCTTCACAGCGGGAGATCGAGGAGTTTCTCAGTGAGGCAGCGTGCATGAAAGACTTCAGCCACCCAAATGTCATTCGACTTCTAGGTGTGTGTATAGAAATGAGCTCTCAAGGCATCCCAAAGCCCATGGTAATTTTACCCTTCATGAAATACGGGGACCTGCATACTTACTTACTTTATTCCCGATTGGAGACAGGACCAAAGCATATTCCTCTGCAGACACTATTGAAGTTCATGGTGGATATTGCCCTGGGAATGGAGTATCTGAGCAACAGGAATTTTCTTCATCGAGATTTAGCTGCTCGAAACTGCATGTTGCGAGATGACATGACTGTCTGTGTTGCGGACTTCGGCCTCTCTAAGAAGATTTACAGTGGCGATTATTACCGCCAAGGCCGCATTGCTAAGATGCCTGTTAAATGGATCGCCATAGAAAGTCTTGCAGACCGAGTCTACACAAGTAAAAGTGATGTGTGGGCATTTGGCGTGACCATGTGGGAAATAGCTACGCGGGGAATGACTCCCTATCCTGGGGTCCAGAACCATGAGATGTATGACTATCTTCTCCATGGCCACAGGTTGAAGCAGCCCGAAGACTGCCTGGATGAACTGTATGAAATAATGTACTCTTGCTGGAGAACCGATCCCTTAGACCGCCCCACCTTTTCAGTATTGAGGCTGCAGCTAGAAAAACTCTTAGAAAGTTTGCCTGACGTTCGGAACCAAGCAGACGTTATTTACGTCAATACACAGTTGCTGGAGAGCTCTGAGGGCCTGGCCCAGGGCTCCACCCTTGCTCCACTGGACTTGAACATCGACCCTGACTCTATAATTGCCTCCTGCACTCCCCGCGCTGCCATCAGTGTGGTCACAGCAGAAGTTCATGACAGCAAACCTCATGAAGGACGGTACATCCTGAATGGGGGCAGTGAGGAATGGGAAGATCTGACTTCTGCCCCCTCTGCTGCAGTCACAGCTGAAAAGAACAGTGTTTTACCGGGGGAGAGACTTGTTAGGAATGGGGTCTCCTGGTCCCATTCGAGCATGCTGCCCTTGGGAAGCTCATTGCCCGATGAACTTTTGTTTGCTGACGACTCCTCAGAAGGCTCAGAAGTCCTGATGTGA',\n",
       "  'ATGGCGGGGCCCGGCTGGGGTCCCCCGCGCCTGGACGGCTTCATCCTCACCGAGCGCCTGGGCAGCGGCACGTACGCCACGGTGTACAAGGCCTACGCCAAGAAGGACACTCGTGAAGTGGTAGCCATAAAGTGTGTAGCCAAGAAAAGTCTGAACAAGGCATCGGTGGAGAACCTCCTCACGGAGATTGAGATCCTCAAGGGCATTCGACATCCCCACATTGTGCAGCTGAAAGACTTTCAGTGGGACAGTGACAATATCTACCTCATCATGGAGTTTTGCGCAGGGGGCGACCTGTCTCGCTTCATCCATACCCGCAGGATTCTGCCTGAGAAGGTGGCGCGTGTCTTCATGCAGCAATTAGCTAGCGCCCTGCAATTCCTGCATGAACGGAATATCTCTCACCTGGATCTGAAGCCACAGAACATTCTACTGAGCTCCTTGGAGAAGCCCCACCTAAAACTGGCAGACTTTGGTTTCGCACAACACATGTCCCCGTGGGATGAGAAGCACGTGCTCCGTGGCTCCCCCCTCTACATGGCCCCCGAGATGGTGTGCCAGCGGCAGTATGACGCCCGCGTGGACCTCTGGTCCATGGGGGTCATCCTGTATGAAGCCCTCTTCGGGCAGCCCCCCTTTGCCTCCAGGTCGTTCTCGGAGCTGGAAGAGAAGATCCGTAGCAACCGGGTCATCGAGCTCCCCTTGCGGCCCCTGCTCTCCCGAGACTGCCGGGACCTACTGCAGCGGCTCCTGGAGCGGGACCCCAGCCGTCGCATCTCCTTCCAGGACTTTTTTGCGCACCCCTGGGTGGACCTGGAGCACATGCCCAGTGGGGAGAGTCTGGGGCGAGCAACCGCCCTGGTGGTGCAGGCTGTGAAGAAAGACCAGGAGGGGGATTCAGCAGCTGCCTTATCACTCTACTGCAAGGCTCTGGACTTCTTTGTACCTGCCCTGCACTATGAAGTGGATGCCCAGCGGAAGGAGGCAATTAAGGCAAAGGTGGGGCAGTACGTGTCCCGGGCTGAGGAGCTCAAGGCCATCGTCTCCTCTTCCAATCAGGCCCTGCTGAGGCAGGGGACCTCTGCCCGAGACCTGCTCAGAGAGATGGCCCGGGACAAGCCACGCCTCCTAGCTGCCCTGGAAGTGGCTTCAGCTGCCATGGCCAAGGAGGAGGCCGCCGGCGGGGAGCAGGATGCCCTGGACCTGTACCAGCACAGCCTGGGGGAGCTACTGCTGTTGCTGGCAGCGGAGCCCCCGGGCCGGAGGCGGGAGCTGCTTCACACTGAGGTTCAGAACCTCATGGCCCGAGCTGAATACTTGAAGGAGCAGGTCAAGATGAGGGAATCTCGCTGGGAAGCTGACACCCTGGACAAAGAGGGACTGTCGGAATCTGTTCGTAGCTCTTGCACCCTTCAGTGA',\n",
       "  'ATGTCGGCCCGGACGCCATTGCCGACGGTGAACGAGCGGGACACGGAAAATCATACATCTGTGGATGGATATACTGAACCACACATCCAGCCTACCAAGTCGAGTAGCAGACAGAACATCCCCCGGTGTAGAAACTCCATTACGTCAGCAACAGATGAACAGCCTCACATTGGAAATTACCGTTTACAAAAAACAATAGGGAAGGGAAATTTTGCCAAAGTCAAATTGGCAAGACACGTTCTAACTGGTAGAGAGGTTGCTGTGAAAATAATAGACAAAACTCAGCTAAATCCTACCAGTCTACAAAAGTTATTTCGAGAAGTACGAATAATGAAGATACTGAATCATCCTAATATAGTAAAATTGTTTGAAGTTATTGAAACAGAGAAGACTCTCTATTTAGTCATGGAATACGCGAGTGGGGGTGAAGTATTTGATTACTTAGTTGCCCATGGAAGAATGAAAGAGAAAGAGGCCCGTGCAAAATTTAGGCAGATTGTATCTGCTGTACAGTATTGTCATCAAAAGTACATTGTTCACCGTGATCTTAAGGCTGAAAACCTTCTCCTTGATGGTGATATGAATATTAAAATTGCTGACTTTGGTTTTAGTAATGAATTTACAGTTGGGAACAAATTGGACACATTTTGTGGAAGCCCACCCTATGCTGCTCCCGAGCTTTTCCAAGGAAAGAAGTATGATGGGCCTGAAGTGGATGTGTGGAGTCTGGGCGTCATTCTCTATACATTAGTCAGTGGCTCCTTGCCTTTCGATGGCCAGAATTTAAAGGAACTGCGAGAGCGAGTTTTACGAGGGAAGTACCGTATTCCCTTCTATATGTCCACAGACTGTGAAAATCTTCTGAAGAAATTATTAGTCCTGAATCCAATAAAGAGAGGCAGCTTGGAACAAATAATGAAAGATCGATGGATGAATGTTGGTCATGAAGAGGAAGAACTAAAGCCATATACTGAGCCTGATCCGGATTTCAATGACACAAAAAGAATAGACATTATGGTCACCATGGGCTTTGCACGAGATGAAATAAATGATGCCTTAATAAATCAGAAGTATGATGAAGTTATGGCTACTTATATTCTTCTAGGTAGAAAACCACCTGAATTTGAAGGTGGTGAATCGTTATCCAGTGGAAACTTGTGTCAGAGGTCCCGGCCCAGTAGTGACTTAAACAACAGCACTCTTCAGTCCCCTGCTCACCTGAAGGTCCAGAGAAGTATCTCAGCAAATCAGAAGCAGCGGCGTTTCAGTGATCATGCTGGTCCATCCATTCCTCCTGCTGTATCATATACCAAAAGACCTCAGGCTAACAGTGTGGAAAGTGAACAGAAAGAGGAGTGGGACAAAGATGTGGCTCGAAAACTTGGCAGCACAACAGTTGGATCAAAAAGCGAGATGACTGCAAGCCCTCTTGTAGGGCCAGAGAGGAAAAAATCTTCAACTATTCCAAGTAACAATGTGTATTCTGGAGGTAGCATGGCAAGAAGGAATACATATGTCTGTGAAAGGACCACAGATCGATACGTAGCATTGCAGAATGGAAAAGACAGCAGCCTTACGGAGATGTCTGTGAGTAGCATATCTTCTGCAGGCTCTTCTGTGGCCTCTGCTGTCCCCTCAGCACGACCCCGCCACCAGAAGTCCATGTCCACTTCTGGTCATCCTATTAAAGTCACACTGCCAACCATTAAAGACGGCTCTGAAGCTTACCGGCCTGGTACAACCCAGAGAGTGCCTGCTGCTTCCCCATCTGCTCACAGTATTAGTACTGCGACTCCAGACCGGACCCGTTTTCCCCGAGGGAGCTCAAGCCGAAGCACTTTCCATGGTGAACAGCTCCGGGAGCGACGCAGCGTTGCTTATAATGGGCCACCTGCTTCACCATCCCATGAAACGGGTGCATTTGCACATGCCAGAAGGGGAACGTCAACTGGTATAATAAGCAAAATCACATCCAAATTTGTTCGCAGGGATCCAAGTGAAGGCGAAGCCAGTGGCAGAACCGACACCTCAAGAAGTACATCAGGGGAACCAAAAGAAAGAGACAAGGAAGAGGGTAAAGATTCTAAGCCGCGTTCTTTGCGGTTCACATGGAGTATGAAGACCACTAGTTCAATGGACCCTAATGACATGATGAGAGAAATCCGAAAAGTGTTAGATGCAAATAACTGTGATTATGAGCAAAAAGAGAGATTTTTGCTTTTCTGTGTCCATGGAGACGCTAGACAGGATAGCCTCGTGCAGTGGGAGATGGAAGTCTGCAAGTTGCCACGACTGTCACTTAATGGGGTTCGCTTCAAGCGAATATCTGGGACATCTATTGCCTTTAAGAACATTGCATCAAAAATAGCAAATGAGCTTAAGCTGTAA',\n",
       "  'ATGGATAAGTACGATGTGATTAAGGCCATCGGGCAAGGTGCCTTCGGGAAAGCATACTTAGCTAAAGGGAAATCAGATAGCAAGCACTGTGTCATAAAAGAGATCAATTTTGAAAAGATGCCCATACAAGAAAAAGAAGCTTCAAAGAAAGAAGTGATTCTTCTGGAAAAGATGAAACATCCCAACATTGTAGCCTTCTTCAATTCATTTCAAGAGAATGGCAGGCTGTTTATTGTAATGGAATATTGTGATGGAGGGGATCTCATGAAAAGGATCAATAGACAACGGGGTGTGTTATTTAGTGAAGATCAGATCCTCGGTTGGTTTGTACAGATTTCTCTAGGACTAAAACATATTCATGACAGGAAGATATTACACAGGGACATAAAAGCTCAGAACATTTTTCTTAGCAAGAACGGAATGGTGGCAAAGCTTGGGGACTTTGGTATAGCAAGAGTCCTGAATAATTCCATGGAACTTGCTCGAACTTGTATTGGAACACCTTACTACCTGTCCCCAGAGATCTGTCAGAATAAACCCTACAACAATAAAACGGATATTTGGTCTCTTGGCTGTGTCTTATATGAGCTCTGCACACTTAAACATCCTTTTGAGGGTAACAACTTACAGCAGCTGGTTCTGAAGATTTGTCAAGCACATTTTGCCCCAATATCTCCGGGGTTTTCTCGTGAGCTCCATTCCTTGATATCTCAGCTCTTTCAAGTATCTCCTCGAGACCGACCATCCATAAATTCCATTTTGAAAAGGCCCTTTTTAGAGAATCTTATTCCCAAATATTTGACTCCTGAGGTCATTCAGGAAGAATTCAGTCACATGCTTATATGCAGAGCAGGAGCGCCAGCTTCTCGACATGCTGGGAAGGTGGTCCAGAAGTGTAAAATACAAAAAGTGAGATTCCAGGGAAAGTGCCCACCAAGATCAAGGATATCTGTGCCAATTAAAAGGAATGCTATATTGCATAGAAATGAATGGAGACCACCAGCTGGAGCCCAGAAGGCCAGATCTATAAAAATGATAGAAAGACCCAAAATTGCTGCTGTCTGTGGACATTATGATTATTATTATGCTCAACTTGATATGCTGAGGAGGAGAGCCCACAAACCAAGTTATCACCCTATTCCTCAAGAAAATACTGGAGTTGAGGATTACGGTCAGGAAACGAGGCATGGTCCATCCCCAAGTCAATGGCCTGCTGAGTACCTTCAGAGAAAATTTGAAGCTCAACAATATAAGTTGAAAGTGGAGAAGCAATTGGGTCTTCGTCCATCTTCTGCCGAGCCAAATTACAACCAGAGACAAGAGCTAAGAAGTAATGGAGAAGAGCCTAGATTCCAGGAGCTGCCATTTAGGAAAAACGAAATGAAGGAACAGGAATATTGGAAGCAGTTAGAGGAAATACGCCAACAGTACCACAATGACATGAAAGAAATTAGAAAGAAGATGGGGAGAGAACCAGAGGAGAACTCAAAAATAAGTCATAAAACCTATTTGGTGAAGAAGAGTAACCTGCCTGTCCATCAAGATGCATCTGAGGGAGAAGCACCTGTGCAGATGGAATTTCGCTCTTGTTGCCCAGGCTGGAGTGCAATGGCACGATCTTGGCTCACCGCAACCTCCGCCTCCCAGGACATTGAAAAAGACTTGAAACAAATGAGGCTTCAGAACACAAAGGAAAGTAAAAATCCAGAACAGAAATATAAAGCTAAGAAGGGGGTAAAATTTGAAATTAATTTAGACAAATGTATTTCTGATGAAAACATCCTCCAAGAGGAAGAGGCAATGGATATACCAAATGAAACTTTGACCTTTGAGGATGGCATGAAGTTTAAGGAATATGAATGTGTAAAGGAGCATGGAGATTATACAGACAAAGCATTTGAAAAACTTCACTGCCCAGAAGCAGGGTTTTCCACGCAGACTGTAGCTGCTGTGGGAAACAGGAGGCAGTGGGATGGAGGAGCGCCTCAGACTCTGCTGCAGATGATGGCAGTGGCCGACATCACCTCCACCTGCCCCACGGGGCCTGACAGTGAGTCTGTGCTTAGCGTCAGTCGTCAGGAAGGGAAGACCAAGGACCCGTACAGCCCAGTGCTCATCCTGATGTGA',\n",
       "  'ATGGCGCTGAGGCGGAGCATGGGGCGGCCGGGGCTCCCGCCGCTGCCGCTGCCGCCGCCACCGCGGCTCGGGCTGCTGCTGGCGGCTCTGGCTTCTCTGCTGCTCCCGGAGTCCGCCGCCGCAGGTCTGAAGCTCATGGGAGCCCCGGTGAAGCTGACAGTGTCTCAGGGGCAGCCGGTGAAGCTCAACTGCAGTGTGGAGGGGATGGAGGAGCCTGACATCCAGTGGGTGAAGGATGGGGCTGTGGTCCAGAACTTGGACCAGTTGTACATCCCAGTCAGCGAGCAGCACTGGATCGGCTTCCTCAGCCTGAAGTCAGTGGAGCGCTCTGACGCCGGCCGGTACTGGTGCCAGGTGGAGGATGGGGGTGAAACCGAGATCTCCCAGCCAGTGTGGCTCACGGTAGAAGGTGTGCCATTTTTCACAGTGGAGCCAAAAGATCTGGCAGTGCCACCCAATGCCCCTTTCCAACTGTCTTGTGAGGCTGTGGGTCCCCCTGAACCTGTTACCATTGTCTGGTGGAGAGGAACTACGAAGATCGGGGGACCCGCTCCCTCTCCATCTGTTTTAAATGTAACAGGGGTGACCCAGAGCACCATGTTTTCCTGTGAAGCTCACAACCTAAAAGGCCTGGCCTCTTCTCGCACAGCCACTGTTCACCTTCAAGCACTGCCTGCAGCCCCCTTCAACATCACCGTGACAAAGCTTTCCAGCAGCAACGCTAGTGTGGCCTGGATGCCAGGTGCTGATGGCCGAGCTCTGCTACAGTCCTGTACAGTTCAGGTGACACAGGCCCCAGGAGGCTGGGAAGTCCTGGCTGTTGTGGTCCCTGTGCCCCCCTTTACCTGCCTGCTCCGGGACCTGGTGCCTGCCACCAACTACAGCCTCAGGGTGCGCTGTGCCAATGCCTTGGGGCCCTCTCCCTATGCTGACTGGGTGCCCTTTCAGACCAAGGGTCTAGCCCCAGCCAGCGCTCCCCAAAACCTCCATGCCATCCGCACAGATTCAGGCCTCATCTTGGAGTGGGAAGAAGTGATCCCCGAGGCCCCTTTGGAAGGCCCCCTGGGACCCTACAAACTGTCCTGGGTTCAAGACAATGGAACCCAGGATGAGCTGACAGTGGAGGGGACCAGGGCCAATTTGACAGGCTGGGATCCCCAAAAGGACCTGATCGTACGTGTGTGCGTCTCCAATGCAGTTGGCTGTGGACCCTGGAGTCAGCCACTGGTGGTCTCTTCTCATGACCGTGCAGGCCAGCAGGGCCCTCCTCACAGCCGCACATCCTGGGTACCTGTGGTCCTTGGTGTGCTAACGGCCCTGGTGACGGCTGCTGCCCTGGCCCTCATCCTGCTTCGAAAGAGACGGAAAGAGACGCGGTTTGGGCAAGCCTTTGACAGTGTCATGGCCCGGGGAGAGCCAGCCGTTCACTTCCGGGCAGCCCGGTCCTTCAATCGAGAAAGGCCCGAGCGCATCGAGGCCACATTGGACAGCTTGGGCATCAGCGATGAACTAAAGGAAAAACTGGAGGATGTGCTCATCCCAGAGCAGCAGTTCACCCTGGGCCGGATGTTGGGCAAAGGAGAGTTTGGTTCAGTGCGGGAGGCCCAGCTGAAGCAAGAGGATGGCTCCTTTGTGAAAGTGGCTGTGAAGATGCTGAAAGCTGACATCATTGCCTCAAGCGACATTGAAGAGTTCCTCAGGGAAGCAGCTTGCATGAAGGAGTTTGACCATCCACACGTGGCCAAACTTGTTGGGGTAAGCCTCCGGAGCAGGGCTAAAGGCCGTCTCCCCATCCCCATGGTCATCTTGCCCTTCATGAAGCATGGGGACCTGCATGCCTTCCTGCTCGCCTCCCGGATTGGGGAGAACCCCTTTAACCTACCCCTCCAGACCCTGATCCGGTTCATGGTGGACATTGCCTGCGGCATGGAGTACCTGAGCTCTCGGAACTTCATCCACCGAGACCTGGCTGCTCGGAATTGCATGCTGGCAGAGGACATGACAGTGTGTGTGGCTGACTTCGGACTCTCCCGGAAGATCTACAGTGGGGACTACTATCGTCAAGGCTGTGCCTCCAAACTGCCTGTCAAGTGGCTGGCCCTGGAGAGCCTGGCCGACAACCTGTATACTGTGCAGAGTGACGTGTGGGCGTTCGGGGTGACCATGTGGGAGATCATGACACGTGGGCAGACGCCATATGCTGGCATCGAAAACGCTGAGATTTACAACTACCTCATTGGCGGGAACCGCCTGAAACAGCCTCCGGAGTGTATGGAGGACGTGTATGATCTCATGTACCAGTGCTGGAGTGCTGACCCCAAGCAGCGCCCGAGCTTTACTTGTCTGCGAATGGAACTGGAGAACATCTTGGGCCAGCTGTCTGTGCTATCTGCCAGCCAGGACCCCTTATACATCAACATCGAGAGAGCTGAGGAGCCCACTGCGGGAGGCAGCCTGGAGCTACCTGGCAGGGATCAGCCCTACAGTGGGGCTGGGGATGGCAGTGGCATGGGGGCAGTGGGTGGCACTCCCAGTGACTGTCGGTACATACTCACCCCCGGAGGGCTGGCTGAGCAGCCAGGGCAGGCAGAGCACCAGCCAGAGAGTCCCCTCAATGAGACACAGAGGCTTTTGCTGCTGCAGCAAGGGCTACTGCCACACAGTAGCTGTTAG',\n",
       "  'ATGGCGGCGTCCTCCCTGGAACAGAAGCTGTCCCGCCTGGAAGCAAAGCTGAAGCAGGAGAACCGGGAGGCCCGGCGGAGGATCGACCTCAACCTGGATATCAGCCCCCAGCGGCCCAGGCCCACCCTGCAGCTCCCGCTGGCCAACGATGGGGGCAGCCGCTCGCCATCCTCAGAGAGCTCCCCGCAGCACCCCACGCCCCCCGCCCGGCCCCGCCACATGCTGGGGCTCCCGTCAACCCTGTTCACACCCCGCAGCATGGAGAGCATTGAGATTGACCAGAAGCTGCAGGAGATCATGAAGCAGACGGGCTACCTGACCATCGGGGGCCAGCGCTACCAGGCAGAAATCAACGACCTGGAGAACTTGGGCGAGATGGGCAGCGGCACCTGCGGCCAGGTGTGGAAGATGCGCTTCCGGAAGACCGGCCACGTCATTGCCGTTAAGCAAATGCGGCGCTCCGGGAACAAGGAGGAGAACAAGCGCATCCTCATGGACCTGGATGTGGTGCTGAAGAGCCACGACTGCCCCTACATCGTGCAGTGCTTTGGGACGTTCATCACCAACACGGACGTCTTCATCGCCATGGAGCTCATGGGCACCTGCGCTGAGAAGCTCAAGAAGCGGATGCAGGGCCCCATCCCCGAGCGCATTCTGGGCAAGATGACAGTGGCGATTGTGAAGGCGCTGTACTACCTGAAGGAGAAGCACGGTGTCATCCACCGCGACGTCAAGCCCTCCAACATCCTGCTGGACGAGCGGGGCCAGATCAAGCTCTGCGACTTCGGCATCAGCGGCCGCCTGGTGGACTCCAAAGCCAAGACGCGGAGCGCCGGCTGTGCCGCCTACATGGCACCCGAGCGCATTGACCCCCCAGACCCCACCAAGCCGGACTATGACATCCGGGCCGACGTATGGAGCCTGGGCATCTCGTTGGTGGAGCTGGCAACAGGACAGTTTCCCTACAAGAACTGCAAGACGGACTTTGAGGTCCTCACCAAAGTCCTACAGGAAGAGCCCCCGCTTCTGCCCGGACACATGGGCTTCTCGGGGGACTTCCAGTCCTTCGTCAAAGACTGCCTTACTAAAGATCACAGGAAGAGACCAAAGTATAATAAGCTACTTGAACACAGCTTCATCAAGCGCTACGAGACGCTGGAGGTGGACGTGGCGTCCTGGTTCAAGGATGTCATGGCGAAGACTGAGTCACCGCGGACTAGCGGCGTCCTGAGCCAGCCCCACCTGCCCTTCTTCAGGTAG',\n",
       "  'ATGCAGGGGGGCAACTCCGGGGTCCGCAAGCGCGAAGAGGAGGGCGACGGGGCTGGGGCTGTGGCTGCGCCGCCGGCCATCGACTTTCCCGCCGAGGGCCCGGACCCCGAATATGACGAATCTGATGTTCCAGCAGAAATCCAGGTGTTAAAAGAACCCCTACAACAGCCAACCTTCCCTTTTGCAGTTGCAAACCAACTCTTGCTGGTTTCTTTGCTGGAGCACTTGAGCCACGTGCATGAACCAAACCCACTTCGTTCAAGACAGGTGTTTAAGCTACTTTGCCAGACGTTTATCAAAATGGGGCTGCTGTCTTCTTTCACTTGTAGTGACGAGTTTAGCTCATTGAGACTACATCACAACAGAGCTATTACTCACTTAATGAGGTCTGCTAAAGAGAGAGTTCGTCAGGATCCTTGTGAGGATATTTCTCGTATCCAGAAAATCAGATCAAGGGAAGTAGCCTTGGAAGCACAAACTTCACGTTACTTAAATGAATTTGAAGAACTTGCCATCTTAGGAAAAGGTGGATACGGAAGAGTATACAAGGTCAGGAATAAATTAGATGGTCAGTATTATGCAATAAAAAAAATCCTGATTAAGGGTGCAACTAAAACAGTTTGCATGAAGGTCCTACGGGAAGTGAAGGTGCTGGCAGGTCTTCAGCACCCCAATATTGTTGGCTATCACACCGCGTGGATAGAACATGTTCATGTGATTCAGCCACGAGCAGACAGAGCTGCCATTGAGTTGCCATCTCTGGAAGTGCTCTCCGACCAGGAAGAGGACAGAGAGCAATGTGGTGTTAAAAATGATGAAAGTAGCAGCTCATCCATTATCTTTGCTGAGCCCACCCCAGAAAAAGAAAAACGCTTTGGAGAATCTGACACTGAAAATCAGAATAACAAGTCGGTGAAGTACACCACCAATTTAGTCATAAGAGAATCTGGTGAACTTGAGTCGACCCTGGAGCTCCAGGAAAATGGCTTGGCTGGTTTGTCTGCCAGTTCAATTGTGGAACAGCAGCTGCCACTCAGGCGTAATTCCCACCTAGAGGAGAGTTTCACATCCACCGAAGAATCTTCCGAAGAAAATGTCAACTTTTTGGGTCAGACAGAGGCACAGTACCACCTGATGCTGCACATCCAGATGCAGCTGTGTGAGCTCTCGCTGTGGGATTGGATAGTCGAGAGAAACAAGCGGGGCCGGGAGTATGTGGACGAGTCTGCCTGTCCTTATGTTATGGCCAATGTTGCAACAAAAATTTTTCAAGAATTGGTAGAAGGTGTGTTTTACATACATAACATGGGAATTGTGCACCGAGATCTGAAGCCAAGAAATATTTTTCTTCATGGCCCTGATCAGCAAGTAAAAATAGGAGACTTTGGTCTGGCCTGCACAGACATCCTACAGAAGAACACAGACTGGACCAACAGAAACGGGAAGAGAACACCAACACATACGTCCAGAGTGGGTACTTGTCTGTACGCTTCACCCGAACAGTTGGAAGGATCTGAGTATGATGCCAAGTCAGATATGTACAGCTTGGGTGTGGTCCTGCTAGAGCTCTTTCAGCCGTTTGGAACAGAAATGGAGCGAGCAGAAGTTCTAACAGGTTTAAGAACTGGTCAGTTGCCGGAATCCCTCCGTAAAAGGTGTCCAGTGCAAGCCAAGTATATCCAGCACTTAACGAGAAGGAACTCATCGCAGAGACCATCTGCCATTCAGCTGCTGCAGAGTGAACTTTTCCAAAATTCTGGAAATGTTAACCTCACCCTACAGATGAAGATAATAGAGCAAGAAAAAGAAATTGCAGAACTAAAGAAGCAGCTAAACCTCCTTTCTCAAGACAAAGGGGTGAGGGATGACGGAAAGGATGGGGGCGTGGGATGA',\n",
       "  'ATGGAGAAATATCATGGTTTAGAAAAAATAGGAGAGGGAACATATGGGGTAGTTTACAAAGCCCAAAATAATTATGGTGAAACTTTTGCCTTAAAAAAAATTAGATTAGAAAAAGAAGATGAAGGAATTCCATCAACAACAATAAGAGAGATTAGCATTTTGAAGGAATTGAAACATTCCAACATAGTAAAGTTGTATGATGTCATACACACAAAGAAAAGATTAGTTTTAGTTTTTGAGCATCTTGATCAAGATCTTAAGAAACTTCTGGATGTTTGTGAAGGAGGGTTGGAATCCGTAACAGCAAAATCATTTTTACTTCAGCTTCTAAATGGCATAGCTTATTGCCATGATCGTCGTGTTTTACACAGAGATTTGAAGCCCCAGAATTTATTAATTAATAGAGAGGGAGAATTAAAAATTGCCGACTTTGGGTTAGCCAGAGCATTTGGAATTCCTGTAAGAAAATATACGCATGAAGTAGTAACCTTATGGTATCGAGCACCAGATGTTTTAATGGGATCTAAGAAATATTCAACTACTATTGATATATGGAGTGTTGGATGTATATTTGCTGAAATGGTAAATGGAACCCCATTATTTCCAGGGGTATCTGAAGCAGACCAATTAATGAGGATATTTAGAATTCTAGGAACTCCTAATTCTAAAAATTGGCCAAATGTAACTGAACTACCAAAGTATGATCCTAATTTTACCGTATATGAACCTTTACCATGGGAATCATTTTTGAAGGGATTAGATGAATCTGGTATCGATTTGCTTTCAAAAATGCTAAAGCTTGATCCAAACCAAAGAATAACAGCAAAACAAGCTCTAGAACATGCGTATTTTAAAGAAAACAATTAA',\n",
       "  'ATGCAGAGCACTTCTAATCATCTGTGGCTTTTATCTGATATTTTAGGCCAAGGAGCTACTGCAAATGTCTTTCGTGGAAGACATAAGAAAACTGGTGATTTATTTGCTATCAAAGTATTTAATAACATAAGCTTCCTTCGTCCAGTGGATGTTCAAATGAGAGAATTTGAAGTGTTGAAAAAACTCAATCACAAAAATATTGTCAAATTATTTGCTATTGAAGAGGAGACAACAACAAGACATAAAGTACTTATTATGGAATTTTGTCCATGTGGGAGTTTATACACTGTTTTAGAAGAACCTTCTAATGCCTATGGACTACCAGAATCTGAATTCTTAATTGTTTTGCGAGATGTGGTGGGTGGAATGAATCATCTACGAGAGAATGGTATAGTGCACCGTGATATCAAGCCAGGAAATATCATGCGTGTTATAGGGGAAGATGGACAGTCTGTGTACAAACTCACAGATTTTGGTGCAGCTAGAGAATTAGAAGATGATGAGCAGTTTGTTTCTCTGTATGGCACAGAAGAATATTTGCACCCTGATATGTATGAGAGAGCAGTGCTAAGAAAAGATCATCAGAAGAAATATGGAGCAACAGTTGATCTTTGGAGCATTGGGGTAACATTTTACCATGCAGCTACTGGATCACTGCCATTTAGACCCTTTGAAGGGCCTCGTAGGAATAAAGAAGTGATGTATAAAATAATTACAGGAAAGCCTTCTGGTGCAATATCTGGAGTACAGAAAGCAGAAAATGGACCAATTGACTGGAGTGGAGACATGCCTGTTTCTTGCAGTCTTTCTCGGGGTCTTCAGGTTCTACTTACCCCTGTTCTTGCAAACATCCTTGAAGCAGATCAGGAAAAGTGTTGGGGTTTTGACCAGTTTTTTGCAGAAACTAGTGATATACTTCACCGAATGGTAATTCATGTTTTTTCGCTACAACAAATGACAGCTCATAAGATTTATATTCATAGCTATAATACTGCTACTATATTTCATGAACTGGTATATAAACAAACCAAAATTATTTCTTCAAATCAAGAACTTATCTACGAAGGGCGACGCTTAGTCTTAGAACCTGGAAGGCTGGCACAACATTTCCCTAAAACTACTGAGGAAAACCCTATATTTGTAGTAAGCCGGGAACCTCTGAATACCATAGGATTAATATATGAAAAAATTTCCCTCCCTAAAGTACATCCACGTTATGATTTAGACGGGGATGCTAGCATGGCTAAGGCAATAACAGGGGTTGTGTGTTATGCCTGCAGAATTGCCAGTACCTTACTGCTTTATCAGGAATTAATGCGAAAGGGGATACGATGGCTGATTGAATTAATTAAAGATGATTACAATGAAACTGTTCACAAAAAGACAGAAGTTGTGATCACATTGGATTTCTGTATCAGAAACATTGAAAAAACTGTGAAAGTATATGAAAAGTTGATGAAGATCAACCTGGAAGCGGCAGAGTTAGGTGAAATTTCAGACATACACACCAAATTGTTGAGACTTTCCAGTTCTCAGGGAACAATAGAAACCAGTCTTCAGGATATCGACAGCAGATTATCTCCAGGTGGATCACTGGCAGACGCATGGGCACATCAAGAAGGCACTCATCCGAAAGACAGAAATGTAGAAAAACTACAAGTCCTGTTAAATTGCATGACAGAGATTTACTATCAGTTCAAAAAAGACAAAGCAGAACGTAGATTAGCTTATAATGAAGAACAAATCCACAAATTTGATAAGCAAAAACTGTATTACCATGCCACAAAAGCTATGACGCACTTTACAGATGAATGTGTTAAAAAGTATGAGGCATTTTTGAATAAGTCAGAAGAATGGATAAGAAAGATGCTTCATCTTAGGAAACAGTTATTATCGCTGACTAATCAGTGTTTTGATATTGAAGAAGAAGTATCAAAATATCAAGAATATACTAATGAGTTACAAGAAACTCTGCCTCAGAAAATGTTTACAGCTTCCAGTGGAATCAAACATACCATGACCCCAATTTATCCAAGTTCTAACACATTAGTAGAAATGACTCTTGGTATGAAGAAATTAAAGGAAGAGATGGAAGGGGTGGTTAAAGAACTTGCTGAAAATAACCACATTTTAGAAAGGTTTGGCTCTTTAACCATGGATGGTGGCCTTCGCAACGTTGACTGTCTTTAG',\n",
       "  'ATGGAGGCGGCGGTCGCTGCTCCGCGTCCCCGGCTGCTCCTCCTCGTGCTGGCGGCGGCGGCGGCGGCGGCGGCGGCGCTGCTCCCGGGGGCGACGGCGTTACAGTGTTTCTGCCACCTCTGTACAAAAGACAATTTTACTTGTGTGACAGATGGGCTCTGCTTTGTCTCTGTCACAGAGACCACAGACAAAGTTATACACAACAGCATGTGTATAGCTGAAATTGACTTAATTCCTCGAGATAGGCCGTTTGTATGTGCACCCTCTTCAAAAACTGGGTCTGTGACTACAACATATTGCTGCAATCAGGACCATTGCAATAAAATAGAACTTCCAACTACTGTAAAGTCATCACCTGGCCTTGGTCCTGTGGAACTGGCAGCTGTCATTGCTGGACCAGTGTGCTTCGTCTGCATCTCACTCATGTTGATGGTCTATATCTGCCACAACCGCACTGTCATTCACCATCGAGTGCCAAATGAAGAGGACCCTTCATTAGATCGCCCTTTTATTTCAGAGGGTACTACGTTGAAAGACTTAATTTATGATATGACAACGTCAGGTTCTGGCTCAGGTTTACCATTGCTTGTTCAGAGAACAATTGCGAGAACTATTGTGTTACAAGAAAGCATTGGCAAAGGTCGATTTGGAGAAGTTTGGAGAGGAAAGTGGCGGGGAGAAGAAGTTGCTGTTAAGATATTCTCCTCTAGAGAAGAACGTTCGTGGTTCCGTGAGGCAGAGATTTATCAAACTGTAATGTTACGTCATGAAAACATCCTGGGATTTATAGCAGCAGACAATAAAGACAATGGTACTTGGACTCAGCTCTGGTTGGTGTCAGATTATCATGAGCATGGATCCCTTTTTGATTACTTAAACAGATACACAGTTACTGTGGAAGGAATGATAAAACTTGCTCTGTCCACGGCGAGCGGTCTTGCCCATCTTCACATGGAGATTGTTGGTACCCAAGGAAAGCCAGCCATTGCTCATAGAGATTTGAAATCAAAGAATATCTTGGTAAAGAAGAATGGAACTTGCTGTATTGCAGACTTAGGACTGGCAGTAAGACATGATTCAGCCACAGATACCATTGATATTGCTCCAAACCACAGAGTGGGAACAAAAAGGTACATGGCCCCTGAAGTTCTCGATGATTCCATAAATATGAAACATTTTGAATCCTTCAAACGTGCTGACATCTATGCAATGGGCTTAGTATTCTGGGAAATTGCTCGACGATGTTCCATTGGTGGAATTCATGAAGATTACCAACTGCCTTATTATGATCTTGTACCTTCTGACCCATCAGTTGAAGAAATGAGAAAAGTTGTTTGTGAACAGAAGTTAAGGCCAAATATCCCAAACAGATGGCAGAGCTGTGAAGCCTTGAGAGTAATGGCTAAAATTATGAGAGAATGTTGGTATGCCAATGGAGCAGCTAGGCTTACAGCATTGCGGATTAAGAAAACATTATCGCAACTCAGTCAACAGGAAGGCATCAAAATGTAA']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5torch\n",
    "\n",
    "dataset = h5torch.Dataset(\n",
    "    \"./data/dataset/DAVIS.h5t\",\n",
    "    sampling=\"coo\",\n",
    "    subset=(\"unstructured/split_cold\", \"valid\"),\n",
    "    in_memory=True,\n",
    ")\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=12, shuffle=True)\n",
    "batch = next(iter(dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Molecule drugs\n",
      "-  Drug_fp torch.Size([12, 2048])\n",
      "-  Drug_emb_graph torch.Size([12, 512])\n",
      "-  Drug_emb_image torch.Size([12, 512])\n",
      "-  Drug_emb_text torch.Size([12, 768])\n",
      "\n",
      "Protein targets\n",
      "-  Target_fp torch.Size([12, 4170])\n",
      "-  Target_emb_ESM torch.Size([12, 1280])\n",
      "-  Target_emb_T5 torch.Size([12, 1024])\n",
      "-  Target_emb_DNA torch.Size([12, 1024])\n",
      "\n",
      "Interaction value:  torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "print(\"Molecule drugs\")\n",
    "for key in [\"0/Drug_fp\", '0/Drug_emb_graph', '0/Drug_emb_image', '0/Drug_emb_text']:\n",
    "    print(\"- \", key[2:], batch[key].shape)\n",
    "\n",
    "print(\"\\nProtein targets\")\n",
    "for key in ['1/Target_fp', '1/Target_emb_ESM', '1/Target_emb_T5', '1/Target_emb_DNA']:\n",
    "    print(\"- \", key[2:], batch[key].shape)\n",
    "\n",
    "print(\"\\nInteraction value: \", batch[\"central\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import h5torch\n",
    "\n",
    "def get_dataset(split_type, split_name):\n",
    "    return h5torch.Dataset(\n",
    "        \"./data/dataset/DAVIS.h5t\",\n",
    "        sampling=\"coo\",\n",
    "        subset=(f\"unstructured/{split_type}\", split_name),\n",
    "        in_memory=True,\n",
    "    )\n",
    "\n",
    "class CustomH5Dataset(Dataset):\n",
    "    def __init__(self, h5_dataset, inputs_0, inputs_1):\n",
    "        self.dataset = h5_dataset\n",
    "        self.inputs_0 = inputs_0\n",
    "        self.inputs_1 = inputs_1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch = self.dataset[idx]\n",
    "        # Inputs for branch 0 (drug)\n",
    "        x0 = [batch[key] for key in self.inputs_0]\n",
    "        # Inputs for branch 1 (target)\n",
    "        x1 = [batch[key] for key in self.inputs_1]\n",
    "        # Interaction value\n",
    "        y = batch['central']\n",
    "        return x0, x1, y\n",
    "\n",
    "config = {\n",
    "        'inputs_0': ['0/Drug_fp'],\n",
    "        'inputs_1': ['1/Target_fp'],\n",
    "        'model_type': 'plain',  # 'plain' or 'variational'\n",
    "}\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = get_dataset(\"split_rand\", 'train')\n",
    "valid_dataset = get_dataset(\"split_rand\", 'valid')\n",
    "test_dataset = get_dataset(\"split_rand\", 'test')\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    CustomH5Dataset(train_dataset, config['inputs_0'], config['inputs_1']),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    CustomH5Dataset(valid_dataset, config['inputs_0'], config['inputs_1']),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    CustomH5Dataset(test_dataset, config['inputs_0'], config['inputs_1']),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment: single_view_fp\n",
      "Epoch [1/10], Train Loss: 6.2590, Valid Loss: 0.4720\n",
      "Epoch [2/10], Train Loss: 0.5535, Valid Loss: 0.3568\n",
      "Epoch [3/10], Train Loss: 0.4491, Valid Loss: 0.3484\n",
      "Epoch [4/10], Train Loss: 0.4225, Valid Loss: 0.4932\n",
      "Epoch [5/10], Train Loss: 0.4021, Valid Loss: 0.5732\n",
      "Epoch [6/10], Train Loss: 0.4050, Valid Loss: 0.6627\n",
      "Epoch [7/10], Train Loss: 0.4071, Valid Loss: 0.3678\n",
      "Epoch [8/10], Train Loss: 0.3850, Valid Loss: 0.4727\n",
      "Epoch [9/10], Train Loss: 0.3837, Valid Loss: 0.4347\n",
      "Epoch [10/10], Train Loss: 0.3859, Valid Loss: 0.3137\n",
      "Test Loss: 0.3159\n",
      "Experiment: single_view_fp, Best Valid Loss: 0.3137, Test Loss: 0.3159\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "os.chdir('/home/robsyc/Desktop/thesis/MB-VAE-DTI/')\n",
    "from utils.modelTraining import train_and_evaluate\n",
    "\n",
    "CONFIGS = {\n",
    "    'single_view_fp': {\n",
    "        'inputs_0': ['0/Drug_fp'],\n",
    "        'inputs_1': ['1/Target_fp'],\n",
    "        'model_type': 'plain',  # 'plain' or 'variational'\n",
    "    },\n",
    "    # 'var_single_view_emb': {\n",
    "    #     'inputs_0': ['0/Drug_emb_graph'],\n",
    "    #     'inputs_1': ['1/Target_emb_T5'],\n",
    "    #     'model_type': 'variational',\n",
    "    # },\n",
    "}\n",
    "\n",
    "split_type = 'split_rand'\n",
    "for exp_name, config in CONFIGS.items():\n",
    "    print(f\"\\nRunning experiment: {exp_name}\")\n",
    "    best_valid_loss, test_loss = train_and_evaluate(\n",
    "        config=config,\n",
    "        split_type=split_type,\n",
    "        num_epochs=10,\n",
    "        batch_size=16\n",
    "    )\n",
    "    print(f\"Experiment: {exp_name}, Best Valid Loss: {best_valid_loss:.4f}, Test Loss: {test_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mbvae_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
