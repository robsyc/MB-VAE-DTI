{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Experiments were conducted using:\n",
    "- `mb_vae_dti/training/run.py`: command line interface for training\n",
    "- and the scripts in `scripts/training/` for running the experiments\n",
    "\n",
    "This notebook shows some plots and analysis of the unsupervised pretraining, general DTI training and benchmark fine-tuning results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting working directory to: /home/robsyc/Desktop/thesis/MB-VAE-DTI\n"
     ]
    }
   ],
   "source": [
    "from resolve import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 02:15:29,434 - mb_vae_dti.validating.analysis - INFO - Loading 64 result files from notebooks/results/baseline_kiba_cold\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/loss              0.3702\n",
      "test/Y_KIBA_ci         0.7335\n",
      "test/Y_KIBA_mse        0.3702\n",
      "test/Y_KIBA_pearson    0.6481\n",
      "test/Y_KIBA_r2         0.3899\n",
      "test/Y_KIBA_rmse       0.6084\n",
      "dtype: float64\n",
      "Params: 15.4M\n"
     ]
    }
   ],
   "source": [
    "# Analyzing collected results\n",
    "\n",
    "from mb_vae_dti.validating.analysis import *\n",
    "\n",
    "df = load_gridsearch_results(\"notebooks/results/baseline_kiba_cold/\")\n",
    "\n",
    "df\n",
    "\n",
    "top_5 = df.sort_values(by=\"best_val_loss\", ascending=True).head(5)\n",
    "print(round(get_test_averages(top_5), 4))\n",
    "print(f\"Params: {top_5['trainable_params'].mean() / 1000000:.1f}M\")\n",
    "\n",
    "# # # get subsets with desired distinction (encoder_type and aggregator_type)\n",
    "# with_concat = df[df[\"config.model.aggregator_type\"] == \"concat\"]\n",
    "# with_attentive = df[df[\"config.model.aggregator_type\"] == \"attentive\"]\n",
    "\n",
    "# # # get 5 best performers\n",
    "# with_concat_best = with_concat.sort_values(by=\"best_val_loss\", ascending=True).head(5)\n",
    "# with_attentive_best = with_attentive.sort_values(by=\"best_val_loss\", ascending=True).head(5)\n",
    "\n",
    "# # # get stats\n",
    "# print(f\"Params: {with_concat_best['trainable_params'].mean() / 1000000:.1f}M\")\n",
    "# print(round(get_test_averages(with_concat_best), 4))\n",
    "# print(\"-\" * 100)\n",
    "# print(f\"Params: {with_attentive_best['trainable_params'].mean() / 1000000:.1f}M\")\n",
    "# print(round(get_test_averages(with_attentive_best), 4))\n",
    "\n",
    "# cols_of_interest = [\"experiment_name\", \"trainable_params\", \"test/real_pKd_mse\", \"test/real_pKi_mse\", \"test/real_KIBA_mse\", \"test/binary_accuracy\", \"test/binary_f1\", \"test/binary_auprc\"]\n",
    "# df_test = df[cols_of_interest].copy()\n",
    "# df_test.columns = [col.replace(\"test/\", \"\") if col.startswith(\"test/\") else col for col in df_test.columns]\n",
    "# print(df_test.to_string(index=False, float_format=\"{:8.4f}\".format))\n",
    "\n",
    "# df = load_gridsearch_results(\"notebooks/results/train_cold/\")\n",
    "# cols_of_interest = [\"experiment_name\", \"trainable_params\", \"test/real_pKd_mse\", \"test/real_pKi_mse\", \"test/real_KIBA_mse\", \"test/binary_accuracy\", \"test/binary_f1\", \"test/binary_auprc\"]\n",
    "# df_test = df[cols_of_interest].copy()\n",
    "# df_test.columns = [col.replace(\"test/\", \"\") if col.startswith(\"test/\") else col for col in df_test.columns]\n",
    "# print(df_test.to_string(index=False, float_format=\"{:8.4f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['experiment_name', 'best_val_loss', 'best_epoch', 'trainable_params',\n",
       "       'val/loss_Y', 'val/loss_Y_pKd', 'val/loss_Y_pKi', 'val/loss_Y_KIBA',\n",
       "       'val/loss', 'val/binary_accuracy', 'val/binary_auprc',\n",
       "       'val/binary_auroc', 'val/binary_f1', 'val/real_pKd_ci',\n",
       "       'val/real_pKd_mse', 'val/real_pKd_pearson', 'val/real_pKd_r2',\n",
       "       'val/real_pKd_rmse', 'val/real_pKi_ci', 'val/real_pKi_mse',\n",
       "       'val/real_pKi_pearson', 'val/real_pKi_r2', 'val/real_pKi_rmse',\n",
       "       'val/real_KIBA_ci', 'val/real_KIBA_mse', 'val/real_KIBA_pearson',\n",
       "       'val/real_KIBA_r2', 'val/real_KIBA_rmse', 'val/loss_drug_contrastive',\n",
       "       'val/loss_target_contrastive', 'val/loss_contrastive',\n",
       "       'val/loss_accuracy', 'test/loss_Y', 'test/loss_Y_pKd',\n",
       "       'test/loss_Y_pKi', 'test/loss_Y_KIBA', 'test/loss',\n",
       "       'test/binary_accuracy', 'test/binary_auprc', 'test/binary_auroc',\n",
       "       'test/binary_f1', 'test/real_pKd_ci', 'test/real_pKd_mse',\n",
       "       'test/real_pKd_pearson', 'test/real_pKd_r2', 'test/real_pKd_rmse',\n",
       "       'test/real_pKi_ci', 'test/real_pKi_mse', 'test/real_pKi_pearson',\n",
       "       'test/real_pKi_r2', 'test/real_pKi_rmse', 'test/real_KIBA_ci',\n",
       "       'test/real_KIBA_mse', 'test/real_KIBA_pearson', 'test/real_KIBA_r2',\n",
       "       'test/real_KIBA_rmse', 'test/loss_drug_contrastive',\n",
       "       'test/loss_target_contrastive', 'test/loss_contrastive',\n",
       "       'test/loss_accuracy', 'total_training_time', 'avg_time_per_epoch',\n",
       "       'total_epochs', 'config.training.learning_rate',\n",
       "       'config.training.scheduler', 'config.loss.weights',\n",
       "       'config.loss.dti_weights', 'config.data.batch_size',\n",
       "       'config.data.h5_path', 'config.data.drug_features',\n",
       "       'config.data.target_features', 'config.model.embedding_dim',\n",
       "       'config.model.hidden_dim', 'config.model.n_layers',\n",
       "       'config.model.dropout', 'config.model.encoder_type',\n",
       "       'config.loss.contrastive_temp', 'config.model.aggregator_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>best_val_loss</th>\n",
       "      <th>best_epoch</th>\n",
       "      <th>trainable_params</th>\n",
       "      <th>val/loss</th>\n",
       "      <th>val/Y_pKd_ci</th>\n",
       "      <th>val/Y_pKd_mse</th>\n",
       "      <th>val/Y_pKd_pearson</th>\n",
       "      <th>val/Y_pKd_r2</th>\n",
       "      <th>val/Y_pKd_rmse</th>\n",
       "      <th>...</th>\n",
       "      <th>config.loss.weights</th>\n",
       "      <th>config.data.batch_size</th>\n",
       "      <th>config.data.h5_path</th>\n",
       "      <th>config.data.drug_features</th>\n",
       "      <th>config.data.target_features</th>\n",
       "      <th>config.model.embedding_dim</th>\n",
       "      <th>config.model.hidden_dim</th>\n",
       "      <th>config.model.n_layers</th>\n",
       "      <th>config.model.dropout</th>\n",
       "      <th>config.model.encoder_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baseline_finetune_DAVIS_cold_b00c0028</td>\n",
       "      <td>0.562607</td>\n",
       "      <td>10</td>\n",
       "      <td>1720576</td>\n",
       "      <td>0.562607</td>\n",
       "      <td>0.761968</td>\n",
       "      <td>0.562607</td>\n",
       "      <td>0.548152</td>\n",
       "      <td>0.287945</td>\n",
       "      <td>0.922116</td>\n",
       "      <td>...</td>\n",
       "      <td>1_0_0_0</td>\n",
       "      <td>16</td>\n",
       "      <td>data/input/dti.h5torch</td>\n",
       "      <td>FP-Morgan</td>\n",
       "      <td>FP-ESP</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>baseline_finetune_DAVIS_cold_b01c0001</td>\n",
       "      <td>0.585412</td>\n",
       "      <td>12</td>\n",
       "      <td>1456640</td>\n",
       "      <td>0.585412</td>\n",
       "      <td>0.746837</td>\n",
       "      <td>0.585412</td>\n",
       "      <td>0.516502</td>\n",
       "      <td>0.259082</td>\n",
       "      <td>0.970364</td>\n",
       "      <td>...</td>\n",
       "      <td>1_0_0_0</td>\n",
       "      <td>16</td>\n",
       "      <td>data/input/dti.h5torch</td>\n",
       "      <td>FP-Morgan</td>\n",
       "      <td>FP-ESP</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>baseline_finetune_DAVIS_cold_b00c0030</td>\n",
       "      <td>0.591870</td>\n",
       "      <td>12</td>\n",
       "      <td>1838272</td>\n",
       "      <td>0.591870</td>\n",
       "      <td>0.748683</td>\n",
       "      <td>0.591870</td>\n",
       "      <td>0.517225</td>\n",
       "      <td>0.250909</td>\n",
       "      <td>1.033440</td>\n",
       "      <td>...</td>\n",
       "      <td>1_0_0_0</td>\n",
       "      <td>16</td>\n",
       "      <td>data/input/dti.h5torch</td>\n",
       "      <td>FP-Morgan</td>\n",
       "      <td>FP-ESP</td>\n",
       "      <td>768</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>transformer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>baseline_finetune_DAVIS_cold_b01c0006</td>\n",
       "      <td>0.592008</td>\n",
       "      <td>44</td>\n",
       "      <td>1838272</td>\n",
       "      <td>0.592008</td>\n",
       "      <td>0.742334</td>\n",
       "      <td>0.592008</td>\n",
       "      <td>0.501378</td>\n",
       "      <td>0.250734</td>\n",
       "      <td>0.916280</td>\n",
       "      <td>...</td>\n",
       "      <td>1_0_0_0</td>\n",
       "      <td>16</td>\n",
       "      <td>data/input/dti.h5torch</td>\n",
       "      <td>FP-Morgan</td>\n",
       "      <td>FP-ESP</td>\n",
       "      <td>768</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>transformer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>baseline_finetune_DAVIS_cold_b01c0023</td>\n",
       "      <td>0.595989</td>\n",
       "      <td>10</td>\n",
       "      <td>1720576</td>\n",
       "      <td>0.595989</td>\n",
       "      <td>0.774765</td>\n",
       "      <td>0.595989</td>\n",
       "      <td>0.560745</td>\n",
       "      <td>0.245695</td>\n",
       "      <td>0.918591</td>\n",
       "      <td>...</td>\n",
       "      <td>1_0_0_0</td>\n",
       "      <td>16</td>\n",
       "      <td>data/input/dti.h5torch</td>\n",
       "      <td>FP-Morgan</td>\n",
       "      <td>FP-ESP</td>\n",
       "      <td>512</td>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          experiment_name  best_val_loss  best_epoch  \\\n",
       "1   baseline_finetune_DAVIS_cold_b00c0028       0.562607          10   \n",
       "34  baseline_finetune_DAVIS_cold_b01c0001       0.585412          12   \n",
       "36  baseline_finetune_DAVIS_cold_b00c0030       0.591870          12   \n",
       "39  baseline_finetune_DAVIS_cold_b01c0006       0.592008          44   \n",
       "62  baseline_finetune_DAVIS_cold_b01c0023       0.595989          10   \n",
       "\n",
       "    trainable_params  val/loss  val/Y_pKd_ci  val/Y_pKd_mse  \\\n",
       "1            1720576  0.562607      0.761968       0.562607   \n",
       "34           1456640  0.585412      0.746837       0.585412   \n",
       "36           1838272  0.591870      0.748683       0.591870   \n",
       "39           1838272  0.592008      0.742334       0.592008   \n",
       "62           1720576  0.595989      0.774765       0.595989   \n",
       "\n",
       "    val/Y_pKd_pearson  val/Y_pKd_r2  val/Y_pKd_rmse  ...  config.loss.weights  \\\n",
       "1            0.548152      0.287945        0.922116  ...              1_0_0_0   \n",
       "34           0.516502      0.259082        0.970364  ...              1_0_0_0   \n",
       "36           0.517225      0.250909        1.033440  ...              1_0_0_0   \n",
       "39           0.501378      0.250734        0.916280  ...              1_0_0_0   \n",
       "62           0.560745      0.245695        0.918591  ...              1_0_0_0   \n",
       "\n",
       "    config.data.batch_size     config.data.h5_path  config.data.drug_features  \\\n",
       "1                       16  data/input/dti.h5torch                  FP-Morgan   \n",
       "34                      16  data/input/dti.h5torch                  FP-Morgan   \n",
       "36                      16  data/input/dti.h5torch                  FP-Morgan   \n",
       "39                      16  data/input/dti.h5torch                  FP-Morgan   \n",
       "62                      16  data/input/dti.h5torch                  FP-Morgan   \n",
       "\n",
       "    config.data.target_features  config.model.embedding_dim  \\\n",
       "1                        FP-ESP                         512   \n",
       "34                       FP-ESP                         512   \n",
       "36                       FP-ESP                         768   \n",
       "39                       FP-ESP                         768   \n",
       "62                       FP-ESP                         512   \n",
       "\n",
       "    config.model.hidden_dim  config.model.n_layers  config.model.dropout  \\\n",
       "1                       128                      3                   0.1   \n",
       "34                      128                      2                   0.1   \n",
       "36                      128                      3                   0.1   \n",
       "39                      128                      3                   0.1   \n",
       "62                      128                      3                   0.1   \n",
       "\n",
       "    config.model.encoder_type  \n",
       "1                      resnet  \n",
       "34                     resnet  \n",
       "36                transformer  \n",
       "39                transformer  \n",
       "62                     resnet  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit distribution\n",
      "X: torch.Size([8])\n",
      "E: torch.Size([5])\n",
      "Number of nodes: tensor([27]) (sampled)\n",
      "Node mask: tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True]])\n",
      "Z_T.X.shape: torch.Size([1, 27, 8])\n",
      "Z_T.E.shape: torch.Size([1, 27, 27, 5])\n",
      "Successfully created molecule: <rdkit.Chem.rdchem.Mol object at 0x757c5cd5a2e0>\n",
      "Molecule SMILES: C.C.CC.cc1CC(C)(o(c)CC)=n234c(-n(c)n)-c(c-2O3)n-12Oc-2(c)(c)c-4\n"
     ]
    }
   ],
   "source": [
    "# Generating a molecule from the limit distribution\n",
    "\n",
    "from mb_vae_dti.training.diffusion.utils import *\n",
    "import json\n",
    "import torch\n",
    "\n",
    "json_path = \"data/processed/molecular_statistics.json\"\n",
    "\n",
    "stats = json.load(open(json_path))\n",
    "\n",
    "dataset_name = \"drugs_cold\"\n",
    "\n",
    "visualization_tools = MolecularVisualization(stats[\"general\"][\"atom_types\"])\n",
    "nodes_dist = DistributionNodes(stats[\"datasets\"][dataset_name][\"node_count_distribution\"])\n",
    "limit_dist = PlaceHolder(\n",
    "    X=torch.tensor(stats[\"datasets\"][dataset_name][\"node_marginals\"]), \n",
    "    E=torch.tensor(stats[\"datasets\"][dataset_name][\"edge_marginals\"]), \n",
    "    y=torch.ones(1) / 1\n",
    ")\n",
    "\n",
    "print(f\"Limit distribution\\nX: {limit_dist.X.shape}\\nE: {limit_dist.E.shape}\")\n",
    "\n",
    "n_nodes = nodes_dist.sample_n(1, device=\"cpu\")\n",
    "print(f\"Number of nodes: {n_nodes} (sampled)\")\n",
    "\n",
    "arange = torch.arange(max(n_nodes), device=\"cpu\").unsqueeze(0).expand(1, -1)\n",
    "node_mask = arange < n_nodes.unsqueeze(1)\n",
    "\n",
    "print(f\"Node mask: {node_mask}\")\n",
    "\n",
    "z_T = sample_discrete_feature_noise(limit_dist=limit_dist, node_mask=node_mask)\n",
    "\n",
    "print(f\"Z_T.X.shape: {z_T.X.shape}\")\n",
    "print(f\"Z_T.E.shape: {z_T.E.shape}\")\n",
    "\n",
    "edge_types = torch.argmax(z_T.E, dim=-1)\n",
    "edge_types = edge_types# - 1\n",
    "node_types = torch.argmax(z_T.X, dim=-1)\n",
    "\n",
    "\n",
    "limit_mol = visualization_tools.mol_from_graphs(node_types[0], edge_types[0])\n",
    "\n",
    "print(f\"Successfully created molecule: {limit_mol}\")\n",
    "if limit_mol is not None:\n",
    "    print(f\"Molecule SMILES: {Chem.MolToSmiles(limit_mol)}\")\n",
    "else:\n",
    "    print(\"Failed to create valid molecule\") \n",
    "\n",
    "\n",
    "import rdkit.Chem as Chem\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "Draw.MolToFile(limit_mol, \"limit_mol.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mb-vae-dti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
