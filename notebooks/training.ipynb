{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Experiments were conducted using:\n",
    "- `mb_vae_dti/training/run.py`: command line interface for training\n",
    "- and the scripts in `scripts/training/` for running the experiments\n",
    "\n",
    "This notebook shows some plots and analysis of the unsupervised pretraining, general DTI training and benchmark fine-tuning results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting working directory to: /home/robsyc/Desktop/thesis/MB-VAE-DTI\n"
     ]
    }
   ],
   "source": [
    "from resolve import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-03 16:46:33,824 - mb_vae_dti.validating.analysis - INFO - Loading 64 result files from notebooks/results/baseline_kiba_rand\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/loss              0.1573\n",
      "test/Y_KIBA_ci         0.8595\n",
      "test/Y_KIBA_mse        0.1573\n",
      "test/Y_KIBA_pearson    0.8749\n",
      "test/Y_KIBA_r2         0.7648\n",
      "test/Y_KIBA_rmse       0.3966\n",
      "dtype: float64\n",
      "Params: 16.9M\n"
     ]
    }
   ],
   "source": [
    "# Analyzing collected results\n",
    "\n",
    "from mb_vae_dti.validating.analysis import *\n",
    "\n",
    "df = load_gridsearch_results(\"notebooks/results/baseline_kiba_rand/\")\n",
    "\n",
    "df\n",
    "\n",
    "top_5 = df.sort_values(by=\"best_val_loss\", ascending=True).head(5)\n",
    "print(round(get_test_averages(top_5), 4))\n",
    "print(f\"Params: {top_5['trainable_params'].mean() / 1000000:.1f}M\")\n",
    "\n",
    "# # # get subsets with desired distinction (encoder_type and aggregator_type)\n",
    "# with_concat = df[df[\"config.model.aggregator_type\"] == \"concat\"]\n",
    "# with_attentive = df[df[\"config.model.aggregator_type\"] == \"attentive\"]\n",
    "\n",
    "# # # get 5 best performers\n",
    "# with_concat_best = with_concat.sort_values(by=\"best_val_loss\", ascending=True).head(5)\n",
    "# with_attentive_best = with_attentive.sort_values(by=\"best_val_loss\", ascending=True).head(5)\n",
    "\n",
    "# # # get stats\n",
    "# print(f\"Params: {with_concat_best['trainable_params'].mean() / 1000000:.1f}M\")\n",
    "# print(round(get_test_averages(with_concat_best), 4))\n",
    "# print(\"-\" * 100)\n",
    "# print(f\"Params: {with_attentive_best['trainable_params'].mean() / 1000000:.1f}M\")\n",
    "# print(round(get_test_averages(with_attentive_best), 4))\n",
    "\n",
    "# cols_of_interest = [\"experiment_name\", \"trainable_params\", \"test/real_pKd_mse\", \"test/real_pKi_mse\", \"test/real_KIBA_mse\", \"test/binary_accuracy\", \"test/binary_f1\", \"test/binary_auprc\"]\n",
    "# df_test = df[cols_of_interest].copy()\n",
    "# df_test.columns = [col.replace(\"test/\", \"\") if col.startswith(\"test/\") else col for col in df_test.columns]\n",
    "# print(df_test.to_string(index=False, float_format=\"{:8.4f}\".format))\n",
    "\n",
    "# df = load_gridsearch_results(\"notebooks/results/train_cold/\")\n",
    "# cols_of_interest = [\"experiment_name\", \"trainable_params\", \"test/real_pKd_mse\", \"test/real_pKi_mse\", \"test/real_KIBA_mse\", \"test/binary_accuracy\", \"test/binary_f1\", \"test/binary_auprc\"]\n",
    "# df_test = df[cols_of_interest].copy()\n",
    "# df_test.columns = [col.replace(\"test/\", \"\") if col.startswith(\"test/\") else col for col in df_test.columns]\n",
    "# print(df_test.to_string(index=False, float_format=\"{:8.4f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>best_val_loss</th>\n",
       "      <th>best_epoch</th>\n",
       "      <th>trainable_params</th>\n",
       "      <th>val/loss</th>\n",
       "      <th>val/Y_KIBA_ci</th>\n",
       "      <th>val/Y_KIBA_mse</th>\n",
       "      <th>val/Y_KIBA_pearson</th>\n",
       "      <th>val/Y_KIBA_r2</th>\n",
       "      <th>val/Y_KIBA_rmse</th>\n",
       "      <th>...</th>\n",
       "      <th>config.loss.weights</th>\n",
       "      <th>config.data.batch_size</th>\n",
       "      <th>config.data.h5_path</th>\n",
       "      <th>config.data.drug_features</th>\n",
       "      <th>config.data.target_features</th>\n",
       "      <th>config.model.embedding_dim</th>\n",
       "      <th>config.model.hidden_dim</th>\n",
       "      <th>config.model.n_layers</th>\n",
       "      <th>config.model.dropout</th>\n",
       "      <th>config.model.encoder_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>baseline_finetune_KIBA_cold_b00c0004</td>\n",
       "      <td>0.333460</td>\n",
       "      <td>50</td>\n",
       "      <td>16841728</td>\n",
       "      <td>0.333460</td>\n",
       "      <td>0.760062</td>\n",
       "      <td>0.333460</td>\n",
       "      <td>0.699008</td>\n",
       "      <td>0.472883</td>\n",
       "      <td>0.688165</td>\n",
       "      <td>...</td>\n",
       "      <td>1_0_0_0</td>\n",
       "      <td>64</td>\n",
       "      <td>data/input/dti.h5torch</td>\n",
       "      <td>FP-Morgan</td>\n",
       "      <td>FP-ESP</td>\n",
       "      <td>1024</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>baseline_finetune_KIBA_cold_b01c0004</td>\n",
       "      <td>0.338392</td>\n",
       "      <td>34</td>\n",
       "      <td>16841728</td>\n",
       "      <td>0.338392</td>\n",
       "      <td>0.753427</td>\n",
       "      <td>0.338392</td>\n",
       "      <td>0.690234</td>\n",
       "      <td>0.464987</td>\n",
       "      <td>0.670500</td>\n",
       "      <td>...</td>\n",
       "      <td>1_0_0_0</td>\n",
       "      <td>32</td>\n",
       "      <td>data/input/dti.h5torch</td>\n",
       "      <td>FP-Morgan</td>\n",
       "      <td>FP-ESP</td>\n",
       "      <td>1024</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>resnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>baseline_finetune_KIBA_cold_b00c0001</td>\n",
       "      <td>0.339152</td>\n",
       "      <td>26</td>\n",
       "      <td>12908544</td>\n",
       "      <td>0.339152</td>\n",
       "      <td>0.752964</td>\n",
       "      <td>0.339152</td>\n",
       "      <td>0.690663</td>\n",
       "      <td>0.463886</td>\n",
       "      <td>0.752744</td>\n",
       "      <td>...</td>\n",
       "      <td>1_0_0_0</td>\n",
       "      <td>64</td>\n",
       "      <td>data/input/dti.h5torch</td>\n",
       "      <td>FP-Morgan</td>\n",
       "      <td>FP-ESP</td>\n",
       "      <td>768</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>transformer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>baseline_finetune_KIBA_cold_b00c0018</td>\n",
       "      <td>0.339814</td>\n",
       "      <td>45</td>\n",
       "      <td>17375488</td>\n",
       "      <td>0.339814</td>\n",
       "      <td>0.753953</td>\n",
       "      <td>0.339814</td>\n",
       "      <td>0.691998</td>\n",
       "      <td>0.462838</td>\n",
       "      <td>0.690825</td>\n",
       "      <td>...</td>\n",
       "      <td>1_0_0_0</td>\n",
       "      <td>64</td>\n",
       "      <td>data/input/dti.h5torch</td>\n",
       "      <td>FP-Morgan</td>\n",
       "      <td>FP-ESP</td>\n",
       "      <td>768</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>transformer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>baseline_finetune_KIBA_cold_b00c0026</td>\n",
       "      <td>0.340133</td>\n",
       "      <td>26</td>\n",
       "      <td>12908544</td>\n",
       "      <td>0.340133</td>\n",
       "      <td>0.752621</td>\n",
       "      <td>0.340133</td>\n",
       "      <td>0.692200</td>\n",
       "      <td>0.462234</td>\n",
       "      <td>0.697769</td>\n",
       "      <td>...</td>\n",
       "      <td>1_0_0_0</td>\n",
       "      <td>32</td>\n",
       "      <td>data/input/dti.h5torch</td>\n",
       "      <td>FP-Morgan</td>\n",
       "      <td>FP-ESP</td>\n",
       "      <td>768</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>transformer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         experiment_name  best_val_loss  best_epoch  \\\n",
       "61  baseline_finetune_KIBA_cold_b00c0004       0.333460          50   \n",
       "28  baseline_finetune_KIBA_cold_b01c0004       0.338392          34   \n",
       "16  baseline_finetune_KIBA_cold_b00c0001       0.339152          26   \n",
       "29  baseline_finetune_KIBA_cold_b00c0018       0.339814          45   \n",
       "33  baseline_finetune_KIBA_cold_b00c0026       0.340133          26   \n",
       "\n",
       "    trainable_params  val/loss  val/Y_KIBA_ci  val/Y_KIBA_mse  \\\n",
       "61          16841728  0.333460       0.760062        0.333460   \n",
       "28          16841728  0.338392       0.753427        0.338392   \n",
       "16          12908544  0.339152       0.752964        0.339152   \n",
       "29          17375488  0.339814       0.753953        0.339814   \n",
       "33          12908544  0.340133       0.752621        0.340133   \n",
       "\n",
       "    val/Y_KIBA_pearson  val/Y_KIBA_r2  val/Y_KIBA_rmse  ...  \\\n",
       "61            0.699008       0.472883         0.688165  ...   \n",
       "28            0.690234       0.464987         0.670500  ...   \n",
       "16            0.690663       0.463886         0.752744  ...   \n",
       "29            0.691998       0.462838         0.690825  ...   \n",
       "33            0.692200       0.462234         0.697769  ...   \n",
       "\n",
       "    config.loss.weights  config.data.batch_size     config.data.h5_path  \\\n",
       "61              1_0_0_0                      64  data/input/dti.h5torch   \n",
       "28              1_0_0_0                      32  data/input/dti.h5torch   \n",
       "16              1_0_0_0                      64  data/input/dti.h5torch   \n",
       "29              1_0_0_0                      64  data/input/dti.h5torch   \n",
       "33              1_0_0_0                      32  data/input/dti.h5torch   \n",
       "\n",
       "    config.data.drug_features  config.data.target_features  \\\n",
       "61                  FP-Morgan                       FP-ESP   \n",
       "28                  FP-Morgan                       FP-ESP   \n",
       "16                  FP-Morgan                       FP-ESP   \n",
       "29                  FP-Morgan                       FP-ESP   \n",
       "33                  FP-Morgan                       FP-ESP   \n",
       "\n",
       "    config.model.embedding_dim  config.model.hidden_dim  \\\n",
       "61                        1024                      512   \n",
       "28                        1024                      512   \n",
       "16                         768                      512   \n",
       "29                         768                      512   \n",
       "33                         768                      512   \n",
       "\n",
       "    config.model.n_layers  config.model.dropout  config.model.encoder_type  \n",
       "61                      3                   0.1                     resnet  \n",
       "28                      3                   0.1                     resnet  \n",
       "16                      2                   0.1                transformer  \n",
       "29                      3                   0.1                transformer  \n",
       "33                      2                   0.1                transformer  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_name</th>\n",
       "      <th>best_val_loss</th>\n",
       "      <th>best_epoch</th>\n",
       "      <th>test/loss</th>\n",
       "      <th>config.data.batch_size</th>\n",
       "      <th>config.model.embedding_dim</th>\n",
       "      <th>config.model.hidden_dim</th>\n",
       "      <th>config.model.n_layers</th>\n",
       "      <th>config.model.dropout</th>\n",
       "      <th>config.training.learning_rate</th>\n",
       "      <th>config.model.encoder_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>baseline_finetune_KIBA_cold_b00c0004</td>\n",
       "      <td>0.333460</td>\n",
       "      <td>50</td>\n",
       "      <td>0.369488</td>\n",
       "      <td>64</td>\n",
       "      <td>1024</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>resnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>baseline_finetune_KIBA_cold_b01c0004</td>\n",
       "      <td>0.338392</td>\n",
       "      <td>34</td>\n",
       "      <td>0.371440</td>\n",
       "      <td>32</td>\n",
       "      <td>1024</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>resnet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>baseline_finetune_KIBA_cold_b00c0001</td>\n",
       "      <td>0.339152</td>\n",
       "      <td>26</td>\n",
       "      <td>0.366906</td>\n",
       "      <td>64</td>\n",
       "      <td>768</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>transformer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>baseline_finetune_KIBA_cold_b00c0018</td>\n",
       "      <td>0.339814</td>\n",
       "      <td>45</td>\n",
       "      <td>0.371739</td>\n",
       "      <td>64</td>\n",
       "      <td>768</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>transformer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>baseline_finetune_KIBA_cold_b00c0026</td>\n",
       "      <td>0.340133</td>\n",
       "      <td>26</td>\n",
       "      <td>0.371350</td>\n",
       "      <td>32</td>\n",
       "      <td>768</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>transformer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         experiment_name  best_val_loss  best_epoch  \\\n",
       "61  baseline_finetune_KIBA_cold_b00c0004       0.333460          50   \n",
       "28  baseline_finetune_KIBA_cold_b01c0004       0.338392          34   \n",
       "16  baseline_finetune_KIBA_cold_b00c0001       0.339152          26   \n",
       "29  baseline_finetune_KIBA_cold_b00c0018       0.339814          45   \n",
       "33  baseline_finetune_KIBA_cold_b00c0026       0.340133          26   \n",
       "\n",
       "    test/loss  config.data.batch_size  config.model.embedding_dim  \\\n",
       "61   0.369488                      64                        1024   \n",
       "28   0.371440                      32                        1024   \n",
       "16   0.366906                      64                         768   \n",
       "29   0.371739                      64                         768   \n",
       "33   0.371350                      32                         768   \n",
       "\n",
       "    config.model.hidden_dim  config.model.n_layers  config.model.dropout  \\\n",
       "61                      512                      3                   0.1   \n",
       "28                      512                      3                   0.1   \n",
       "16                      512                      2                   0.1   \n",
       "29                      512                      3                   0.1   \n",
       "33                      512                      2                   0.1   \n",
       "\n",
       "    config.training.learning_rate config.model.encoder_type  \n",
       "61                         0.0005                    resnet  \n",
       "28                         0.0005                    resnet  \n",
       "16                         0.0005               transformer  \n",
       "29                         0.0005               transformer  \n",
       "33                         0.0005               transformer  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=\"best_val_loss\", ascending=True)[\n",
    "    ['experiment_name', 'best_val_loss', 'best_epoch', 'test/loss', 'config.data.batch_size',\n",
    "     'config.model.embedding_dim', 'config.model.hidden_dim', \n",
    "     'config.model.n_layers', 'config.model.dropout', 'config.training.learning_rate',\n",
    "     'config.model.encoder_type']#, 'config.model.aggregator_type']\n",
    "    ].head(5)\n",
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['experiment_name', 'best_val_loss', 'best_epoch', 'trainable_params',\n",
       "       'val/loss', 'val/Y_pKd_ci', 'val/Y_pKd_mse', 'val/Y_pKd_pearson',\n",
       "       'val/Y_pKd_r2', 'val/Y_pKd_rmse', 'test/loss', 'test/Y_pKd_ci',\n",
       "       'test/Y_pKd_mse', 'test/Y_pKd_pearson', 'test/Y_pKd_r2',\n",
       "       'test/Y_pKd_rmse', 'total_training_time', 'avg_time_per_epoch',\n",
       "       'total_epochs', 'config.training.learning_rate',\n",
       "       'config.training.scheduler', 'config.loss.weights',\n",
       "       'config.data.batch_size', 'config.data.h5_path',\n",
       "       'config.data.drug_features', 'config.data.target_features',\n",
       "       'config.model.embedding_dim', 'config.model.hidden_dim',\n",
       "       'config.model.n_layers', 'config.model.dropout',\n",
       "       'config.model.encoder_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limit distribution\n",
      "X: torch.Size([8])\n",
      "E: torch.Size([5])\n",
      "Number of nodes: tensor([27]) (sampled)\n",
      "Node mask: tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
      "         True, True, True]])\n",
      "Z_T.X.shape: torch.Size([1, 27, 8])\n",
      "Z_T.E.shape: torch.Size([1, 27, 27, 5])\n",
      "Successfully created molecule: <rdkit.Chem.rdchem.Mol object at 0x757c5cd5a2e0>\n",
      "Molecule SMILES: C.C.CC.cc1CC(C)(o(c)CC)=n234c(-n(c)n)-c(c-2O3)n-12Oc-2(c)(c)c-4\n"
     ]
    }
   ],
   "source": [
    "# Generating a molecule from the limit distribution\n",
    "\n",
    "from mb_vae_dti.training.diffusion.utils import *\n",
    "import json\n",
    "import torch\n",
    "\n",
    "json_path = \"data/processed/molecular_statistics.json\"\n",
    "\n",
    "stats = json.load(open(json_path))\n",
    "\n",
    "dataset_name = \"drugs_cold\"\n",
    "\n",
    "visualization_tools = MolecularVisualization(stats[\"general\"][\"atom_types\"])\n",
    "nodes_dist = DistributionNodes(stats[\"datasets\"][dataset_name][\"node_count_distribution\"])\n",
    "limit_dist = PlaceHolder(\n",
    "    X=torch.tensor(stats[\"datasets\"][dataset_name][\"node_marginals\"]), \n",
    "    E=torch.tensor(stats[\"datasets\"][dataset_name][\"edge_marginals\"]), \n",
    "    y=torch.ones(1) / 1\n",
    ")\n",
    "\n",
    "print(f\"Limit distribution\\nX: {limit_dist.X.shape}\\nE: {limit_dist.E.shape}\")\n",
    "\n",
    "n_nodes = nodes_dist.sample_n(1, device=\"cpu\")\n",
    "print(f\"Number of nodes: {n_nodes} (sampled)\")\n",
    "\n",
    "arange = torch.arange(max(n_nodes), device=\"cpu\").unsqueeze(0).expand(1, -1)\n",
    "node_mask = arange < n_nodes.unsqueeze(1)\n",
    "\n",
    "print(f\"Node mask: {node_mask}\")\n",
    "\n",
    "z_T = sample_discrete_feature_noise(limit_dist=limit_dist, node_mask=node_mask)\n",
    "\n",
    "print(f\"Z_T.X.shape: {z_T.X.shape}\")\n",
    "print(f\"Z_T.E.shape: {z_T.E.shape}\")\n",
    "\n",
    "edge_types = torch.argmax(z_T.E, dim=-1)\n",
    "edge_types = edge_types# - 1\n",
    "node_types = torch.argmax(z_T.X, dim=-1)\n",
    "\n",
    "\n",
    "limit_mol = visualization_tools.mol_from_graphs(node_types[0], edge_types[0])\n",
    "\n",
    "print(f\"Successfully created molecule: {limit_mol}\")\n",
    "if limit_mol is not None:\n",
    "    print(f\"Molecule SMILES: {Chem.MolToSmiles(limit_mol)}\")\n",
    "else:\n",
    "    print(\"Failed to create valid molecule\") \n",
    "\n",
    "\n",
    "import rdkit.Chem as Chem\n",
    "from rdkit.Chem import Draw\n",
    "\n",
    "Draw.MolToFile(limit_mol, \"limit_mol.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mb-vae-dti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
